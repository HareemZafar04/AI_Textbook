"use strict";(globalThis.webpackChunkai_textbook=globalThis.webpackChunkai_textbook||[]).push([[8377],{5512:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>p,frontMatter:()=>a,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"ml/reinforcement-learning","title":"Reinforcement Learning","description":"Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by interacting with an environment. The agent receives rewards or penalties based on its actions and aims to maximize the cumulative reward over time through trial and error.","source":"@site/docs/ml/reinforcement-learning.md","sourceDirName":"ml","slug":"/ml/reinforcement-learning","permalink":"/ai-textbook/docs/ml/reinforcement-learning","draft":false,"unlisted":false,"editUrl":"https://github.com/ai-textbook/ai-textbook/edit/main/docs/ml/reinforcement-learning.md","tags":[],"version":"current","frontMatter":{"sidebar_label":"Reinforcement Learning"},"sidebar":"tutorialSidebar","previous":{"title":"Unsupervised Learning","permalink":"/ai-textbook/docs/ml/unsupervised-learning"},"next":{"title":"Deep Learning","permalink":"/ai-textbook/docs/ml/deep-learning"}}');var s=t(4848),r=t(8453);const a={sidebar_label:"Reinforcement Learning"},l="Reinforcement Learning",o={},d=[{value:"Overview of Reinforcement Learning",id:"overview-of-reinforcement-learning",level:2},{value:"Key Concepts in RL",id:"key-concepts-in-rl",level:2},{value:"1. The Agent-Environment Interface",id:"1-the-agent-environment-interface",level:3},{value:"2. Exploration vs. Exploitation",id:"2-exploration-vs-exploitation",level:3},{value:"Types of Reinforcement Learning",id:"types-of-reinforcement-learning",level:2},{value:"1. Model-Free vs. Model-Based",id:"1-model-free-vs-model-based",level:3},{value:"Model-Free RL",id:"model-free-rl",level:4},{value:"Q-Learning Example",id:"q-learning-example",level:5},{value:"Deep Q-Network (DQN)",id:"deep-q-network-dqn",level:4},{value:"2. Policy-Based Methods",id:"2-policy-based-methods",level:3},{value:"Policy Gradient Example (REINFORCE)",id:"policy-gradient-example-reinforce",level:4},{value:"3. Actor-Critic Methods",id:"3-actor-critic-methods",level:3},{value:"RL Environments and Applications",id:"rl-environments-and-applications",level:2},{value:"1. Classic Control Problems",id:"1-classic-control-problems",level:3},{value:"2. Multi-Armed Bandit Problem",id:"2-multi-armed-bandit-problem",level:3},{value:"Advanced RL Concepts",id:"advanced-rl-concepts",level:2},{value:"1. Deep Reinforcement Learning",id:"1-deep-reinforcement-learning",level:3},{value:"2. Continuous Action Spaces",id:"2-continuous-action-spaces",level:3},{value:"RL Applications",id:"rl-applications",level:2},{value:"1. Game Playing",id:"1-game-playing",level:3},{value:"2. Robotics",id:"2-robotics",level:3},{value:"3. Finance",id:"3-finance",level:3},{value:"4. Healthcare",id:"4-healthcare",level:3},{value:"Challenges in Reinforcement Learning",id:"challenges-in-reinforcement-learning",level:2},{value:"Future Directions",id:"future-directions",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"reinforcement-learning",children:"Reinforcement Learning"})}),"\n",(0,s.jsx)(n.p,{children:"Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by interacting with an environment. The agent receives rewards or penalties based on its actions and aims to maximize the cumulative reward over time through trial and error."}),"\n",(0,s.jsx)(n.h2,{id:"overview-of-reinforcement-learning",children:"Overview of Reinforcement Learning"}),"\n",(0,s.jsx)(n.p,{children:"In reinforcement learning, an agent learns to achieve a goal by interacting with an environment. The key components are:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Agent"}),": The learner or decision-maker"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Environment"}),": Everything the agent interacts with"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"State"}),": The current situation of the agent"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action"}),": What the agent can do"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reward"}),": Feedback from the environment"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Policy"}),": The agent's strategy for choosing actions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Value Function"}),": Prediction of future rewards"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"key-concepts-in-rl",children:"Key Concepts in RL"}),"\n",(0,s.jsx)(n.h3,{id:"1-the-agent-environment-interface",children:"1. The Agent-Environment Interface"}),"\n",(0,s.jsx)(n.p,{children:"The fundamental interaction loop:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"The environment provides state S to the agent"}),"\n",(0,s.jsx)(n.li,{children:"The agent selects action A based on its policy"}),"\n",(0,s.jsx)(n.li,{children:"The environment transitions to new state S' and provides reward R"}),"\n",(0,s.jsx)(n.li,{children:"The process repeats"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"2-exploration-vs-exploitation",children:"2. Exploration vs. Exploitation"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Exploration"}),": Trying new actions to discover their effects"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Exploitation"}),": Using known information to maximize reward"]}),"\n",(0,s.jsx)(n.li,{children:"Balancing these is crucial for effective learning"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"types-of-reinforcement-learning",children:"Types of Reinforcement Learning"}),"\n",(0,s.jsx)(n.h3,{id:"1-model-free-vs-model-based",children:"1. Model-Free vs. Model-Based"}),"\n",(0,s.jsx)(n.h4,{id:"model-free-rl",children:"Model-Free RL"}),"\n",(0,s.jsx)(n.p,{children:"The agent learns directly from experience without modeling the environment."}),"\n",(0,s.jsx)(n.h5,{id:"q-learning-example",children:"Q-Learning Example"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import numpy as np\nimport random\nfrom collections import defaultdict\n\nclass QLearningAgent:\n    def __init__(self, actions, learning_rate=0.1, discount_factor=0.95, epsilon=0.1):\n        self.actions = actions\n        self.learning_rate = learning_rate\n        self.discount_factor = discount_factor\n        self.epsilon = epsilon  # Exploration rate\n        self.q_table = defaultdict(lambda: np.zeros(len(actions)))\n    \n    def choose_action(self, state):\n        # Epsilon-greedy action selection\n        if random.uniform(0, 1) < self.epsilon:\n            # Explore: random action\n            return random.choice(self.actions)\n        else:\n            # Exploit: best known action\n            state_actions = self.q_table[state]\n            return self.actions[np.argmax(state_actions)]\n    \n    def learn(self, state, action, reward, next_state):\n        current_q = self.q_table[state][self.actions.index(action)]\n        \n        # Calculate maximum future reward\n        max_next_q = np.max(self.q_table[next_state])\n        \n        # Update Q-value using Bellman equation\n        new_q = current_q + self.learning_rate * (\n            reward + self.discount_factor * max_next_q - current_q\n        )\n        \n        self.q_table[state][self.actions.index(action)] = new_q\n\n# Simple grid world example\nclass GridWorld:\n    def __init__(self, width=5, height=5):\n        self.width = width\n        self.height = height\n        self.start = (0, 0)\n        self.goal = (4, 4)\n        self.state = self.start\n        self.actions = ['up', 'down', 'left', 'right']\n        \n    def reset(self):\n        self.state = self.start\n        return self.state\n    \n    def step(self, action):\n        x, y = self.state\n        \n        # Update position based on action\n        if action == 'up' and y > 0:\n            y -= 1\n        elif action == 'down' and y < self.height - 1:\n            y += 1\n        elif action == 'left' and x > 0:\n            x -= 1\n        elif action == 'right' and x < self.width - 1:\n            x += 1\n            \n        self.state = (x, y)\n        \n        # Calculate reward\n        if self.state == self.goal:\n            reward = 100  # Goal reached\n            done = True\n        elif self.state == (2, 2):  # Penalty position\n            reward = -10\n            done = False\n        else:\n            reward = -1  # Time penalty\n            done = False\n            \n        return self.state, reward, done\n\n# Train the agent\nenv = GridWorld()\nagent = QLearningAgent(env.actions)\n\n# Training loop\nfor episode in range(1000):\n    state = env.reset()\n    total_reward = 0\n    \n    while True:\n        action = agent.choose_action(state)\n        next_state, reward, done = env.step(action)\n        \n        agent.learn(state, action, reward, next_state)\n        \n        state = next_state\n        total_reward += reward\n        \n        if done:\n            break\n    \n    # Decay exploration rate\n    if agent.epsilon > 0.01:\n        agent.epsilon *= 0.995\n\nprint(\"Training complete!\")\nprint(\"Final Q-table for a few states:\")\nfor state in [(0, 0), (1, 1), (2, 2), (3, 3), (4, 4)]:\n    print(f\"State {state}: {dict(zip(env.actions, agent.q_table[state]))}\")\n"})}),"\n",(0,s.jsx)(n.h4,{id:"deep-q-network-dqn",children:"Deep Q-Network (DQN)"}),"\n",(0,s.jsx)(n.p,{children:"Combines Q-learning with deep neural networks."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport random\nfrom collections import deque\n\nclass DQN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(DQN, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.fc3 = nn.Linear(hidden_size, output_size)\n        self.relu = nn.ReLU()\n        \n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nclass DQNAgent:\n    def __init__(self, state_size, action_size, hidden_size=64):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.hidden_size = hidden_size\n        \n        # Neural networks\n        self.q_network = DQN(state_size, hidden_size, action_size)\n        self.target_network = DQN(state_size, hidden_size, action_size)\n        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.001)\n        \n        # Experience replay\n        self.memory = deque(maxlen=10000)\n        self.epsilon = 1.0  # Exploration rate\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.995\n        self.batch_size = 32\n        self.update_target_freq = 100\n        self.step_count = 0\n        \n    def remember(self, state, action, reward, next_state, done):\n        self.memory.append((state, action, reward, next_state, done))\n    \n    def choose_action(self, state):\n        if random.random() <= self.epsilon:\n            return random.randrange(self.action_size)\n        \n        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n        q_values = self.q_network(state_tensor)\n        return np.argmax(q_values.cpu().data.numpy())\n    \n    def replay(self):\n        if len(self.memory) < self.batch_size:\n            return\n            \n        batch = random.sample(self.memory, self.batch_size)\n        states = torch.FloatTensor([e[0] for e in batch])\n        actions = torch.LongTensor([e[1] for e in batch])\n        rewards = torch.FloatTensor([e[2] for e in batch])\n        next_states = torch.FloatTensor([e[3] for e in batch])\n        dones = torch.BoolTensor([e[4] for e in batch])\n        \n        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n        next_q_values = self.target_network(next_states).max(1)[0].detach()\n        target_q_values = rewards + (0.95 * next_q_values * ~dones)\n        \n        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)\n        \n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        \n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n    \n    def update_target_network(self):\n        self.target_network.load_state_dict(self.q_network.state_dict())\n\n# Example usage would involve training the DQN on an environment\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-policy-based-methods",children:"2. Policy-Based Methods"}),"\n",(0,s.jsx)(n.p,{children:"Instead of learning a value function, these methods learn a policy directly."}),"\n",(0,s.jsx)(n.h4,{id:"policy-gradient-example-reinforce",children:"Policy Gradient Example (REINFORCE)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\n\nclass PolicyNetwork(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(PolicyNetwork, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.fc3 = nn.Linear(hidden_size, output_size)\n        self.softmax = nn.Softmax(dim=-1)\n        \n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = self.fc3(x)\n        return self.softmax(x)\n\nclass PolicyGradientAgent:\n    def __init__(self, state_size, action_size, hidden_size=32):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.network = PolicyNetwork(state_size, hidden_size, action_size)\n        self.optimizer = optim.Adam(self.network.parameters(), lr=0.01)\n        self.log_probs = []\n        self.rewards = []\n        \n    def select_action(self, state):\n        state = torch.FloatTensor(state).unsqueeze(0)\n        probs = self.network(state)\n        action_dist = torch.distributions.Categorical(probs)\n        action = action_dist.sample()\n        \n        # Store log probability for gradient computation\n        self.log_probs.append(action_dist.log_prob(action))\n        return action.item()\n    \n    def store_reward(self, reward):\n        self.rewards.append(reward)\n    \n    def update_policy(self):\n        # Compute discounted rewards\n        discounted_rewards = []\n        running_add = 0\n        for reward in reversed(self.rewards):\n            running_add = reward + 0.99 * running_add\n            discounted_rewards.insert(0, running_add)\n        \n        # Normalize rewards\n        discounted_rewards = torch.FloatTensor(discounted_rewards)\n        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9)\n        \n        # Compute loss\n        policy_loss = []\n        for log_prob, reward in zip(self.log_probs, discounted_rewards):\n            policy_loss.append(-log_prob * reward)\n        \n        # Update network\n        self.optimizer.zero_grad()\n        policy_loss = torch.cat(policy_loss).sum()\n        policy_loss.backward()\n        self.optimizer.step()\n        \n        # Reset for next episode\n        self.log_probs = []\n        self.rewards = []\n"})}),"\n",(0,s.jsx)(n.h3,{id:"3-actor-critic-methods",children:"3. Actor-Critic Methods"}),"\n",(0,s.jsx)(n.p,{children:"Combine value-based and policy-based approaches."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass ActorCritic(nn.Module):\n    def __init__(self, state_size, action_size, hidden_size=32):\n        super(ActorCritic, self).__init__()\n        \n        # Shared layers\n        self.shared = nn.Sequential(\n            nn.Linear(state_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU()\n        )\n        \n        # Actor (policy) head\n        self.actor = nn.Sequential(\n            nn.Linear(hidden_size, action_size),\n            nn.Softmax(dim=-1)\n        )\n        \n        # Critic (value) head\n        self.critic = nn.Linear(hidden_size, 1)\n    \n    def forward(self, state):\n        shared_out = self.shared(state)\n        action_probs = self.actor(shared_out)\n        state_value = self.critic(shared_out)\n        return action_probs, state_value\n\nclass ActorCriticAgent:\n    def __init__(self, state_size, action_size, hidden_size=32):\n        self.network = ActorCritic(state_size, action_size, hidden_size)\n        self.optimizer = optim.Adam(self.network.parameters(), lr=0.001)\n    \n    def select_action(self, state):\n        state = torch.FloatTensor(state).unsqueeze(0)\n        action_probs, state_value = self.network(state)\n        action_dist = torch.distributions.Categorical(action_probs)\n        action = action_dist.sample()\n        \n        return action.item(), state_value, action_dist.log_prob(action)\n    \n    def update(self, state, action, reward, next_state, done):\n        state = torch.FloatTensor(state).unsqueeze(0)\n        next_state = torch.FloatTensor(next_state).unsqueeze(0)\n        \n        action_probs, current_value = self.network(state)\n        _, next_value = self.network(next_state)\n        \n        # Calculate advantage\n        target_value = reward + 0.99 * next_value * (1 - done)\n        advantage = target_value - current_value\n        \n        # Calculate actor and critic losses\n        action_dist = torch.distributions.Categorical(action_probs)\n        log_prob = action_dist.log_prob(torch.tensor([action]))\n        actor_loss = -log_prob * advantage.detach()\n        critic_loss = advantage.pow(2)\n        \n        # Update network\n        total_loss = actor_loss + critic_loss\n        self.optimizer.zero_grad()\n        total_loss.backward()\n        self.optimizer.step()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"rl-environments-and-applications",children:"RL Environments and Applications"}),"\n",(0,s.jsx)(n.h3,{id:"1-classic-control-problems",children:"1. Classic Control Problems"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import gym\n\ndef cartpole_example():\n    # CartPole environment from OpenAI Gym\n    env = gym.make('CartPole-v1')\n    \n    # Simple random agent\n    observation = env.reset()\n    total_reward = 0\n    \n    for step in range(1000):\n        env.render()  # Comment out if running without display\n        \n        # Choose random action\n        action = env.action_space.sample()\n        \n        # Take action\n        observation, reward, done, info = env.step(action)\n        total_reward += reward\n        \n        if done:\n            print(f\"Episode finished after {step+1} timesteps with total reward: {total_reward}\")\n            break\n    \n    env.close()\n\n# Run the example\n# cartpole_example()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-multi-armed-bandit-problem",children:"2. Multi-Armed Bandit Problem"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import numpy as np\nimport matplotlib.pyplot as plt\n\nclass MultiArmedBandit:\n    def __init__(self, n_arms):\n        # True reward probabilities for each arm\n        self.probabilities = np.random.uniform(0, 1, n_arms)\n        self.n_arms = n_arms\n        \n    def pull_arm(self, arm):\n        # Return reward based on arm's probability\n        return 1 if np.random.random() < self.probabilities[arm] else 0\n\nclass EpsilonGreedyAgent:\n    def __init__(self, n_arms, epsilon=0.1):\n        self.n_arms = n_arms\n        self.epsilon = epsilon\n        self.counts = np.zeros(n_arms)\n        self.values = np.zeros(n_arms)\n        \n    def select_action(self):\n        if np.random.random() > self.epsilon:\n            # Exploit: choose best arm\n            return np.argmax(self.values)\n        else:\n            # Explore: choose random arm\n            return np.random.randint(self.n_arms)\n    \n    def update(self, arm, reward):\n        self.counts[arm] += 1\n        # Update value using incremental formula\n        self.values[arm] += (reward - self.values[arm]) / self.counts[arm]\n\ndef run_bandit_experiment(n_arms=10, n_steps=1000):\n    bandit = MultiArmedBandit(n_arms)\n    agent = EpsilonGreedyAgent(n_arms)\n    \n    rewards = []\n    optimal_action_count = 0\n    true_optimal_arm = np.argmax(bandit.probabilities)\n    \n    for step in range(n_steps):\n        arm = agent.select_action()\n        reward = bandit.pull_arm(arm)\n        agent.update(arm, reward)\n        \n        rewards.append(reward)\n        \n        if arm == true_optimal_arm:\n            optimal_action_count += 1\n    \n    print(f\"Best arm: {true_optimal_arm}, estimated probability: {agent.values[true_optimal_arm]:.3f}\")\n    print(f\"True probability: {bandit.probabilities[true_optimal_arm]:.3f}\")\n    print(f\"Optimal action selected {optimal_action_count}/{n_steps} times\")\n    \n    # Plot results\n    plt.figure(figsize=(12, 4))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(np.cumsum(rewards) / (np.arange(len(rewards)) + 1))\n    plt.title('Average Reward Over Time')\n    plt.xlabel('Steps')\n    plt.ylabel('Average Reward')\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(agent.values, 'o-', label='Estimated Values')\n    plt.plot(bandit.probabilities, 's-', label='True Values')\n    plt.title('Estimated vs True Arm Values')\n    plt.xlabel('Arm')\n    plt.ylabel('Value')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()\n\nrun_bandit_experiment()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"advanced-rl-concepts",children:"Advanced RL Concepts"}),"\n",(0,s.jsx)(n.h3,{id:"1-deep-reinforcement-learning",children:"1. Deep Reinforcement Learning"}),"\n",(0,s.jsx)(n.p,{children:"Combining RL with deep neural networks for complex tasks."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport numpy as np\n\nclass DeepNetwork(nn.Module):\n    def __init__(self, input_size, output_size, hidden_sizes=[64, 64]):\n        super(DeepNetwork, self).__init__()\n        \n        layers = []\n        prev_size = input_size\n        \n        for hidden_size in hidden_sizes:\n            layers.append(nn.Linear(prev_size, hidden_size))\n            layers.append(nn.ReLU())\n            prev_size = hidden_size\n        \n        layers.append(nn.Linear(prev_size, output_size))\n        \n        self.network = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        return self.network(x)\n\n# Example of a deep network for a complex RL task\n# This would typically be used in more sophisticated algorithms like PPO, A3C, etc.\ndef create_deep_rl_model(state_size, action_size):\n    return DeepNetwork(state_size, action_size, hidden_sizes=[128, 64, 32])\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-continuous-action-spaces",children:"2. Continuous Action Spaces"}),"\n",(0,s.jsx)(n.p,{children:"For environments with continuous action spaces, algorithms like Deep Deterministic Policy Gradient (DDPG) are used."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\n\nclass Actor(nn.Module):\n    def __init__(self, state_size, action_size, hidden_size=64):\n        super(Actor, self).__init__()\n        self.fc1 = nn.Linear(state_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.fc3 = nn.Linear(hidden_size, action_size)\n        self.relu = nn.ReLU()\n        self.tanh = nn.Tanh()\n        \n    def forward(self, state):\n        x = self.relu(self.fc1(state))\n        x = self.relu(self.fc2(x))\n        action = self.tanh(self.fc3(x))  # Output between -1 and 1\n        return action\n\nclass Critic(nn.Module):\n    def __init__(self, state_size, action_size, hidden_size=64):\n        super(Critic, self).__init__()\n        self.fc1 = nn.Linear(state_size + action_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.fc3 = nn.Linear(hidden_size, 1)\n        self.relu = nn.ReLU()\n        \n    def forward(self, state, action):\n        x = torch.cat([state, action], dim=1)\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        value = self.fc3(x)\n        return value\n"})}),"\n",(0,s.jsx)(n.h2,{id:"rl-applications",children:"RL Applications"}),"\n",(0,s.jsx)(n.h3,{id:"1-game-playing",children:"1. Game Playing"}),"\n",(0,s.jsx)(n.p,{children:"RL agents have achieved superhuman performance in games like:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Go (AlphaGo)"}),"\n",(0,s.jsx)(n.li,{children:"Chess and Shogi (AlphaZero)"}),"\n",(0,s.jsx)(n.li,{children:"Atari games (DQN)"}),"\n",(0,s.jsx)(n.li,{children:"Dota 2 and StarCraft II"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"2-robotics",children:"2. Robotics"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Manipulation tasks"}),"\n",(0,s.jsx)(n.li,{children:"Navigation"}),"\n",(0,s.jsx)(n.li,{children:"Adaptive control"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"3-finance",children:"3. Finance"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Portfolio management"}),"\n",(0,s.jsx)(n.li,{children:"Algorithmic trading"}),"\n",(0,s.jsx)(n.li,{children:"Risk assessment"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"4-healthcare",children:"4. Healthcare"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Treatment optimization"}),"\n",(0,s.jsx)(n.li,{children:"Drug discovery"}),"\n",(0,s.jsx)(n.li,{children:"Personalized medicine"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"challenges-in-reinforcement-learning",children:"Challenges in Reinforcement Learning"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sample Efficiency"}),": Often requiring many interactions with the environment"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Exploration"}),": Balancing exploration of unknown states with exploitation of known good actions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Credit Assignment"}),": Determining which actions were responsible for rewards"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Non-stationarity"}),": Environment changing as the agent learns"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Scalability"}),": Handling high-dimensional state and action spaces"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety"}),": Ensuring agents don't take dangerous actions during learning"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"future-directions",children:"Future Directions"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Meta-learning"}),": Agents that learn to learn quickly"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multi-agent RL"}),": Coordination between multiple agents"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Transfer Learning"}),": Applying learned policies to new domains"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Explainable RL"}),": Understanding and interpreting agent decisions"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Reinforcement learning continues to advance rapidly, with new algorithms and applications emerging regularly. Its ability to learn from interaction makes it particularly suitable for complex decision-making problems."})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>l});var i=t(6540);const s={},r=i.createContext(s);function a(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);
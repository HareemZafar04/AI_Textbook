"use strict";(globalThis.webpackChunkai_textbook=globalThis.webpackChunkai_textbook||[]).push([[1232],{4003:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>o,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"nlp/language-models","title":"Language Models","description":"Language models are fundamental components in Natural Language Processing that assign probabilities to sequences of words. They are used to predict the likelihood of a word given the preceding context or to generate new text that resembles human language.","source":"@site/docs/nlp/language-models.md","sourceDirName":"nlp","slug":"/nlp/language-models","permalink":"/ai-textbook/docs/nlp/language-models","draft":false,"unlisted":false,"editUrl":"https://github.com/ai-textbook/ai-textbook/edit/main/docs/nlp/language-models.md","tags":[],"version":"current","frontMatter":{"sidebar_label":"Language Models"},"sidebar":"tutorialSidebar","previous":{"title":"Text Processing Techniques","permalink":"/ai-textbook/docs/nlp/text-processing"},"next":{"title":"NLP Applications","permalink":"/ai-textbook/docs/nlp/applications"}}');var r=t(4848),i=t(8453);const o={sidebar_label:"Language Models"},a="Language Models",l={},d=[{value:"What is a Language Model?",id:"what-is-a-language-model",level:2},{value:"Types of Language Models",id:"types-of-language-models",level:2},{value:"1. N-gram Models",id:"1-n-gram-models",level:3},{value:"Unigram Model (N=1)",id:"unigram-model-n1",level:4},{value:"Bigram Model (N=2)",id:"bigram-model-n2",level:4},{value:"Trigram Model (N=3) and Beyond",id:"trigram-model-n3-and-beyond",level:4},{value:"2. Neural Language Models",id:"2-neural-language-models",level:3},{value:"Recurrent Neural Networks (RNNs) for Language Modeling",id:"recurrent-neural-networks-rnns-for-language-modeling",level:4},{value:"Long Short-Term Memory (LSTM) Models",id:"long-short-term-memory-lstm-models",level:4},{value:"3. Transformer-Based Models",id:"3-transformer-based-models",level:3},{value:"Attention Mechanism",id:"attention-mechanism",level:4},{value:"Pre-trained Language Models",id:"pre-trained-language-models",level:2},{value:"1. BERT (Bidirectional Encoder Representations from Transformers)",id:"1-bert-bidirectional-encoder-representations-from-transformers",level:3},{value:"2. GPT (Generative Pre-trained Transformer)",id:"2-gpt-generative-pre-trained-transformer",level:3},{value:"Evaluating Language Models",id:"evaluating-language-models",level:2},{value:"1. Perplexity",id:"1-perplexity",level:3},{value:"2. BLEU Score",id:"2-bleu-score",level:3},{value:"Applications of Language Models",id:"applications-of-language-models",level:2},{value:"Challenges in Language Modeling",id:"challenges-in-language-modeling",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"language-models",children:"Language Models"})}),"\n",(0,r.jsx)(n.p,{children:"Language models are fundamental components in Natural Language Processing that assign probabilities to sequences of words. They are used to predict the likelihood of a word given the preceding context or to generate new text that resembles human language."}),"\n",(0,r.jsx)(n.h2,{id:"what-is-a-language-model",children:"What is a Language Model?"}),"\n",(0,r.jsx)(n.p,{children:"A language model estimates the probability of a sequence of words occurring in a given language. Formally, for a sequence of words W = (w\u2081, w\u2082, ..., w\u2099), a language model estimates:"}),"\n",(0,r.jsx)(n.p,{children:"P(W) = P(w\u2081, w\u2082, ..., w\u2099) = P(w\u2081) \xd7 P(w\u2082|w\u2081) \xd7 P(w\u2083|w\u2081, w\u2082) \xd7 ... \xd7 P(w\u2099|w\u2081, w\u2082, ..., w\u2099\u208b\u2081)"}),"\n",(0,r.jsx)(n.p,{children:"This probability distribution over sequences allows the model to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Predict the next word in a sequence"}),"\n",(0,r.jsx)(n.li,{children:"Generate new text"}),"\n",(0,r.jsx)(n.li,{children:"Evaluate the fluency of a sentence"}),"\n",(0,r.jsx)(n.li,{children:"Power applications like machine translation and speech recognition"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"types-of-language-models",children:"Types of Language Models"}),"\n",(0,r.jsx)(n.h3,{id:"1-n-gram-models",children:"1. N-gram Models"}),"\n",(0,r.jsx)(n.p,{children:'N-gram models are classical statistical models that predict the next word based on the previous N-1 words. The "N" refers to the number of previous words used as context.'}),"\n",(0,r.jsx)(n.h4,{id:"unigram-model-n1",children:"Unigram Model (N=1)"}),"\n",(0,r.jsx)(n.p,{children:"Considers each word independently:\nP(w\u1d62) = count(w\u1d62) / total word count"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from collections import Counter\nimport math\n\nclass UnigramModel:\n    def __init__(self, texts):\n        # Tokenize and flatten all texts\n        all_tokens = []\n        for text in texts:\n            tokens = text.lower().split()\n            all_tokens.extend(tokens)\n        \n        # Count word frequencies\n        self.word_counts = Counter(all_tokens)\n        self.total_words = len(all_tokens)\n        \n    def word_probability(self, word):\n        return self.word_counts.get(word, 0) / self.total_words\n    \n    def sentence_probability(self, sentence):\n        tokens = sentence.lower().split()\n        prob = 1.0\n        for token in tokens:\n            prob *= self.word_probability(token)\n        return prob\n\n# Example usage\ntexts = [\n    "the cat sat on the mat",\n    "the dog ran in the park",\n    "cats and dogs are pets"\n]\n\nmodel = UnigramModel(texts)\nprob = model.sentence_probability("the cat ran")\nprint(f"Probability: {prob}")\n'})}),"\n",(0,r.jsx)(n.h4,{id:"bigram-model-n2",children:"Bigram Model (N=2)"}),"\n",(0,r.jsx)(n.p,{children:"Considers the previous word:\nP(w\u1d62|w\u1d62\u208b\u2081) = count(w\u1d62\u208b\u2081, w\u1d62) / count(w\u1d62\u208b\u2081)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class BigramModel:\n    def __init__(self, texts):\n        self.bigram_counts = Counter()\n        self.word_counts = Counter()\n        \n        for text in texts:\n            tokens = text.lower().split()\n            for i in range(len(tokens)):\n                self.word_counts[tokens[i]] += 1\n                \n                if i > 0:\n                    bigram = (tokens[i-1], tokens[i])\n                    self.bigram_counts[bigram] += 1\n    \n    def bigram_probability(self, prev_word, current_word):\n        bigram = (prev_word, current_word)\n        if self.word_counts[prev_word] == 0:\n            return 0\n        return self.bigram_counts.get(bigram, 0) / self.word_counts[prev_word]\n    \n    def sentence_probability(self, sentence):\n        tokens = sentence.lower().split()\n        if len(tokens) == 0:\n            return 1.0\n        \n        prob = self.word_counts[tokens[0]] / sum(self.word_counts.values())  # P(first_word)\n        \n        for i in range(1, len(tokens)):\n            prob *= self.bigram_probability(tokens[i-1], tokens[i])\n        \n        return prob\n\n# Example usage\nmodel = BigramModel(texts)\nprob = model.sentence_probability("the cat sat")\nprint(f"Bigram probability: {prob}")\n'})}),"\n",(0,r.jsx)(n.h4,{id:"trigram-model-n3-and-beyond",children:"Trigram Model (N=3) and Beyond"}),"\n",(0,r.jsx)(n.p,{children:"Considers the previous two words (for trigrams):\nP(w\u1d62|w\u1d62\u208b\u2082, w\u1d62\u208b\u2081) = count(w\u1d62\u208b\u2082, w\u1d62\u208b\u2081, w\u1d62) / count(w\u1d62\u208b\u2082, w\u1d62\u208b\u2081)"}),"\n",(0,r.jsx)(n.h3,{id:"2-neural-language-models",children:"2. Neural Language Models"}),"\n",(0,r.jsx)(n.p,{children:"Neural language models use neural networks to capture complex patterns in text."}),"\n",(0,r.jsx)(n.h4,{id:"recurrent-neural-networks-rnns-for-language-modeling",children:"Recurrent Neural Networks (RNNs) for Language Modeling"}),"\n",(0,r.jsx)(n.p,{children:"RNNs process sequences by maintaining a hidden state that captures information about the sequence seen so far."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass RNNLanguageModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n        super(RNNLanguageModel, self).__init__()\n        \n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        \n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n        self.dropout = nn.Dropout(0.3)\n        \n    def forward(self, x, hidden=None):\n        batch_size = x.size(0)\n        \n        # Initialize hidden state\n        if hidden is None:\n            h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)\n        else:\n            h0 = hidden\n        \n        # Embedding\n        embedded = self.dropout(self.embedding(x))\n        \n        # RNN\n        rnn_out, hidden_out = self.rnn(embedded, h0)\n        \n        # Linear layer to vocab size\n        output = self.fc(self.dropout(rnn_out))\n        \n        return output, hidden_out\n\n# Example usage would involve training with sequences of text data\n"})}),"\n",(0,r.jsx)(n.h4,{id:"long-short-term-memory-lstm-models",children:"Long Short-Term Memory (LSTM) Models"}),"\n",(0,r.jsx)(n.p,{children:"LSTMs address the vanishing gradient problem in RNNs, allowing for better modeling of long-term dependencies."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"class LSTMLanguageModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n        super(LSTMLanguageModel, self).__init__()\n        \n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        \n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n        self.dropout = nn.Dropout(0.3)\n        \n    def forward(self, x, hidden=None):\n        batch_size = x.size(0)\n        \n        # Initialize hidden state\n        if hidden is None:\n            h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)\n            c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)\n        else:\n            h0, c0 = hidden\n        \n        # Embedding\n        embedded = self.dropout(self.embedding(x))\n        \n        # LSTM\n        lstm_out, hidden_out = self.lstm(embedded, (h0, c0))\n        \n        # Linear layer to vocab size\n        output = self.fc(self.dropout(lstm_out))\n        \n        return output, hidden_out\n"})}),"\n",(0,r.jsx)(n.h3,{id:"3-transformer-based-models",children:"3. Transformer-Based Models"}),"\n",(0,r.jsx)(n.p,{children:"Transformers use self-attention mechanisms to process input sequences in parallel, making them highly efficient and effective."}),"\n",(0,r.jsx)(n.h4,{id:"attention-mechanism",children:"Attention Mechanism"}),"\n",(0,r.jsx)(n.p,{children:"The attention mechanism allows the model to focus on different parts of the input when generating each output token."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\n\nclass Attention(nn.Module):\n    def __init__(self, hidden_dim):\n        super(Attention, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n        self.v = nn.Parameter(torch.rand(hidden_dim))\n        \n    def forward(self, hidden, encoder_outputs):\n        # hidden: [batch_size, hidden_dim]\n        # encoder_outputs: [batch_size, seq_len, hidden_dim]\n        \n        batch_size = encoder_outputs.size(0)\n        seq_len = encoder_outputs.size(1)\n        \n        # Repeat hidden state for each time step\n        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n        \n        # Concatenate hidden and encoder outputs\n        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), 2)))\n        \n        # Calculate attention energies\n        energy = energy.transpose(2, 1)  # [batch_size, hidden_dim, seq_len]\n        v = self.v.repeat(batch_size, 1).unsqueeze(1)  # [batch_size, 1, hidden_dim]\n        energy = torch.bmm(v, energy).squeeze(1)  # [batch_size, seq_len]\n        \n        # Apply softmax to get attention weights\n        return torch.softmax(energy, dim=1)\n\nclass TransformerLanguageModel(nn.Module):\n    def __init__(self, vocab_size, d_model, nhead, num_layers, dim_feedforward):\n        super(TransformerLanguageModel, self).__init__()\n        \n        self.d_model = d_model\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        \n        # Positional encoding\n        self.pos_encoder = PositionalEncoding(d_model)\n        \n        # Transformer decoder layers\n        decoder_layer = nn.TransformerDecoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=dim_feedforward,\n            batch_first=True\n        )\n        self.transformer = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n        \n        self.fc_out = nn.Linear(d_model, vocab_size)\n        self.dropout = nn.Dropout(0.1)\n        \n    def forward(self, tgt, tgt_mask=None):\n        # tgt: [batch_size, seq_len]\n        tgt_emb = self.dropout(self.embedding(tgt) * math.sqrt(self.d_model))\n        tgt_emb = self.pos_encoder(tgt_emb)\n        \n        output = self.transformer(tgt_emb, tgt_emb, tgt_mask)\n        output = self.fc_out(output)\n        \n        return output\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        \n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1).float()\n        \n        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n                            -(math.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        self.register_buffer('pe', pe.unsqueeze(0))\n        \n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]\n"})}),"\n",(0,r.jsx)(n.h2,{id:"pre-trained-language-models",children:"Pre-trained Language Models"}),"\n",(0,r.jsx)(n.h3,{id:"1-bert-bidirectional-encoder-representations-from-transformers",children:"1. BERT (Bidirectional Encoder Representations from Transformers)"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Contextual bidirectional model"}),"\n",(0,r.jsx)(n.li,{children:"Uses masked language modeling during training"}),"\n",(0,r.jsx)(n.li,{children:"Excellent for understanding context"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"2-gpt-generative-pre-trained-transformer",children:"2. GPT (Generative Pre-trained Transformer)"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Autoregressive language model"}),"\n",(0,r.jsx)(n.li,{children:"Excels at text generation"}),"\n",(0,r.jsx)(n.li,{children:"Based on decoder-only transformer architecture"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from transformers import GPT2LMHeadModel, GPT2Tokenizer\n\ndef generate_text(prompt, max_length=50):\n    # Load pre-trained model and tokenizer\n    model = GPT2LMHeadModel.from_pretrained('gpt2')\n    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n    \n    # Tokenize input\n    inputs = tokenizer.encode(prompt, return_tensors='pt')\n    \n    # Generate text\n    outputs = model.generate(\n        inputs,\n        max_length=max_length,\n        num_return_sequences=1,\n        no_repeat_ngram_size=2,\n        do_sample=True,\n        temperature=0.8\n    )\n    \n    # Decode output\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return generated_text\n\n# Example usage\n# result = generate_text(\"The future of artificial intelligence\", max_length=100)\n# print(result)\n"})}),"\n",(0,r.jsx)(n.h2,{id:"evaluating-language-models",children:"Evaluating Language Models"}),"\n",(0,r.jsx)(n.h3,{id:"1-perplexity",children:"1. Perplexity"}),"\n",(0,r.jsx)(n.p,{children:"Perplexity is the standard metric for evaluating language models:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"def calculate_perplexity(model, data_loader):\n    model.eval()\n    total_loss = 0\n    total_tokens = 0\n    \n    with torch.no_grad():\n        for batch in data_loader:\n            inputs, targets = batch\n            outputs = model(inputs)\n            \n            # Calculate loss\n            loss = nn.CrossEntropyLoss(reduction='sum')(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n            total_loss += loss.item()\n            total_tokens += targets.numel()\n    \n    avg_loss = total_loss / total_tokens\n    perplexity = math.exp(avg_loss)\n    return perplexity\n"})}),"\n",(0,r.jsx)(n.h3,{id:"2-bleu-score",children:"2. BLEU Score"}),"\n",(0,r.jsx)(n.p,{children:"For generative models, BLEU measures how similar generated text is to reference text."}),"\n",(0,r.jsx)(n.h2,{id:"applications-of-language-models",children:"Applications of Language Models"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Text Generation"}),": Creating new text that resembles training data"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Machine Translation"}),": Converting text from one language to another"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Text Summarization"}),": Condensing long documents into shorter summaries"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Question Answering"}),": Finding answers within a text"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sentiment Analysis"}),": Determining the emotional tone of text"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Named Entity Recognition"}),": Identifying entities in text"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Chatbots and Virtual Assistants"}),": Providing conversational interfaces"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"challenges-in-language-modeling",children:"Challenges in Language Modeling"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Computational Complexity"}),": Training large models requires significant computational resources"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Data Requirements"}),": High-quality, diverse training data is essential"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Bias and Fairness"}),": Models can perpetuate biases present in training data"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Context Length"}),": Handling long sequences of text remains challenging"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Interpretability"}),": Understanding how models make decisions is difficult"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Language models have evolved from simple n-gram models to sophisticated neural architectures, enabling increasingly human-like text generation and understanding capabilities."})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>a});var s=t(6540);const r={},i=s.createContext(r);function o(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);
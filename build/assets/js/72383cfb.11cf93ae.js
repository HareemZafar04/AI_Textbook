"use strict";(globalThis.webpackChunkai_textbook=globalThis.webpackChunkai_textbook||[]).push([[5283],{4006:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>m,frontMatter:()=>r,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"nlp/text-processing","title":"Text Processing Techniques","description":"Text processing forms the foundation of Natural Language Processing applications. This section explores the essential techniques used to clean, transform, and prepare textual data for analysis and modeling.","source":"@site/docs/nlp/text-processing.md","sourceDirName":"nlp","slug":"/nlp/text-processing","permalink":"/ai-textbook/docs/nlp/text-processing","draft":false,"unlisted":false,"editUrl":"https://github.com/ai-textbook/ai-textbook/edit/main/docs/nlp/text-processing.md","tags":[],"version":"current","frontMatter":{"sidebar_label":"Text Processing Techniques"},"sidebar":"tutorialSidebar","previous":{"title":"Introduction to NLP","permalink":"/ai-textbook/docs/nlp/introduction"},"next":{"title":"Language Models","permalink":"/ai-textbook/docs/nlp/language-models"}}');var o=t(4848),s=t(8453);const r={sidebar_label:"Text Processing Techniques"},a="Text Processing Techniques",l={},d=[{value:"Text Preprocessing Pipeline",id:"text-preprocessing-pipeline",level:2},{value:"1. Text Cleaning",id:"1-text-cleaning",level:2},{value:"Removing Special Characters and Numbers",id:"removing-special-characters-and-numbers",level:3},{value:"Handling HTML Entities and Tags",id:"handling-html-entities-and-tags",level:3},{value:"2. Text Normalization",id:"2-text-normalization",level:2},{value:"Case Normalization",id:"case-normalization",level:3},{value:"Accent Removal",id:"accent-removal",level:3},{value:"Expanding Contractions",id:"expanding-contractions",level:3},{value:"3. Tokenization",id:"3-tokenization",level:2},{value:"Sentence Tokenization",id:"sentence-tokenization",level:3},{value:"Word Tokenization",id:"word-tokenization",level:3},{value:"Subword Tokenization",id:"subword-tokenization",level:3},{value:"4. Stop Words Removal",id:"4-stop-words-removal",level:2},{value:"5. Stemming and Lemmatization",id:"5-stemming-and-lemmatization",level:2},{value:"Stemming",id:"stemming",level:3},{value:"Lemmatization",id:"lemmatization",level:3},{value:"6. Advanced Text Processing",id:"6-advanced-text-processing",level:2},{value:"N-gram Generation",id:"n-gram-generation",level:3},{value:"Text Normalization Pipeline",id:"text-normalization-pipeline",level:3},{value:"7. Handling Special Cases",id:"7-handling-special-cases",level:2},{value:"Handling Negations",id:"handling-negations",level:3},{value:"Spell Correction",id:"spell-correction",level:3},{value:"Best Practices for Text Processing",id:"best-practices-for-text-processing",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"text-processing-techniques",children:"Text Processing Techniques"})}),"\n",(0,o.jsx)(n.p,{children:"Text processing forms the foundation of Natural Language Processing applications. This section explores the essential techniques used to clean, transform, and prepare textual data for analysis and modeling."}),"\n",(0,o.jsx)(n.h2,{id:"text-preprocessing-pipeline",children:"Text Preprocessing Pipeline"}),"\n",(0,o.jsx)(n.p,{children:"The typical text preprocessing pipeline involves multiple sequential steps to convert raw text into a format suitable for computational analysis:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Text Cleaning"}),": Removing unwanted characters and formatting"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Normalization"}),": Converting text to a standard format"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Tokenization"}),": Breaking text into individual units"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Noise Reduction"}),": Removing irrelevant information"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Feature Engineering"}),": Converting text to numerical representations"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"1-text-cleaning",children:"1. Text Cleaning"}),"\n",(0,o.jsx)(n.p,{children:"Text cleaning involves removing unwanted characters, formatting, and artifacts from the raw text:"}),"\n",(0,o.jsx)(n.h3,{id:"removing-special-characters-and-numbers",children:"Removing Special Characters and Numbers"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import re\nimport string\n\ndef clean_text(text):\n    # Remove URLs\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    \n    # Remove user mentions and hashtags (for social media text)\n    text = re.sub(r'@\\w+|#\\w+', '', text)\n    \n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    \n    # Remove extra whitespace\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text\n\n# Example\nraw_text = \"Check out this website: https://example.com! It's amazing @user #nlp\"\ncleaned_text = clean_text(raw_text)\nprint(cleaned_text)  # \"Check out this website  Its amazing  nlp\"\n"})}),"\n",(0,o.jsx)(n.h3,{id:"handling-html-entities-and-tags",children:"Handling HTML Entities and Tags"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"from html import unescape\nimport re\n\ndef remove_html_tags(text):\n    # Remove HTML tags\n    clean_text = re.sub(r'<[^>]+>', '', text)\n    \n    # Decode HTML entities\n    clean_text = unescape(clean_text)\n    \n    return clean_text\n\n# Example\nhtml_text = \"This is &lt;em&gt;important&lt;/em&gt; content.\"\ncleaned = remove_html_tags(html_text)\nprint(cleaned)  # \"This is <em>important</em> content.\"\n"})}),"\n",(0,o.jsx)(n.h2,{id:"2-text-normalization",children:"2. Text Normalization"}),"\n",(0,o.jsx)(n.p,{children:"Text normalization standardizes text to reduce variations in the same concept:"}),"\n",(0,o.jsx)(n.h3,{id:"case-normalization",children:"Case Normalization"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'def normalize_case(text):\n    # Convert to lowercase\n    return text.lower()\n\n# Example\ntext = "This is MIXED case text."\nnormalized = normalize_case(text)\nprint(normalized)  # "this is mixed case text."\n'})}),"\n",(0,o.jsx)(n.h3,{id:"accent-removal",children:"Accent Removal"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import unicodedata\n\ndef remove_accents(text):\n    # Normalize to NFD (decomposed form)\n    nfd = unicodedata.normalize('NFD', text)\n    # Filter out combining characters (accents)\n    without_accents = ''.join(char for char in nfd if unicodedata.category(char) != 'Mn')\n    return without_accents\n\n# Example\ntext = \"Caf\xe9 r\xe9sum\xe9 na\xefve\"\ncleaned = remove_accents(text)\nprint(cleaned)  # \"Cafe resume naive\"\n"})}),"\n",(0,o.jsx)(n.h3,{id:"expanding-contractions",children:"Expanding Contractions"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import re\n\ndef expand_contractions(text):\n    # Common contractions with their expansions\n    contractions = {\n        "don\'t": "do not",\n        "won\'t": "will not",\n        "can\'t": "cannot",\n        "n\'t": " not",\n        "\'re": " are",\n        "\'ve": " have",\n        "\'ll": " will",\n        "\'d": " would",\n        "\'m": " am"\n    }\n    \n    # Create regex pattern for contractions\n    pattern = re.compile(\'|\'.join(re.escape(key) for key in contractions.keys()))\n    \n    # Replace contractions\n    expanded = pattern.sub(lambda match: contractions[match.group(0)], text)\n    return expanded\n\n# Example\ntext = "I can\'t believe it\'s not butter. You\'re awesome!"\nexpanded = expand_contractions(text)\nprint(expanded)  # "I cannot believe it is not butter. You are awesome!"\n'})}),"\n",(0,o.jsx)(n.h2,{id:"3-tokenization",children:"3. Tokenization"}),"\n",(0,o.jsx)(n.p,{children:"Tokenization is the process of breaking text into smaller units (tokens):"}),"\n",(0,o.jsx)(n.h3,{id:"sentence-tokenization",children:"Sentence Tokenization"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import nltk\nfrom nltk.tokenize import sent_tokenize\nnltk.download('punkt')\n\ndef sentence_tokenize(text):\n    sentences = sent_tokenize(text)\n    return sentences\n\n# Example\ntext = \"Hello world. This is a sentence. Here's another one!\"\nsentences = sentence_tokenize(text)\nprint(sentences)  # ['Hello world.', 'This is a sentence.', \"Here's another one!\"]\n"})}),"\n",(0,o.jsx)(n.h3,{id:"word-tokenization",children:"Word Tokenization"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"from nltk.tokenize import word_tokenize\n\ndef word_tokenize_custom(text):\n    tokens = word_tokenize(text)\n    return tokens\n\n# Without NLTK (simple split)\ndef simple_word_tokenize(text):\n    # Split by whitespace and punctuation\n    tokens = re.findall(r'\\b\\w+\\b', text)\n    return tokens\n\n# Example\ntext = \"Hello, world! This is a sample text.\"\ntokens = word_tokenize_custom(text)\nprint(tokens)  # ['Hello', ',', 'world', '!', 'This', 'is', 'a', 'sample', 'text', '.']\n\nsimple_tokens = simple_word_tokenize(text)\nprint(simple_tokens)  # ['Hello', 'world', 'This', 'is', 'a', 'sample', 'text']\n"})}),"\n",(0,o.jsx)(n.h3,{id:"subword-tokenization",children:"Subword Tokenization"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Using Hugging Face tokenizers for BPE (Byte-Pair Encoding)\nfrom transformers import AutoTokenizer\n\ndef subword_tokenize(text, model_name=\"bert-base-uncased\"):\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    tokens = tokenizer.tokenize(text)\n    return tokens\n\n# Example\ntext = \"Tokenization is important for NLP.\"\ntokens = subword_tokenize(text)\nprint(tokens)  # ['token', '##ization', 'is', 'important', 'for', 'nl', '##p', '.']\n"})}),"\n",(0,o.jsx)(n.h2,{id:"4-stop-words-removal",children:"4. Stop Words Removal"}),"\n",(0,o.jsx)(n.p,{children:"Stop words are common words that often carry little semantic meaning:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"from nltk.corpus import stopwords\nimport nltk\nnltk.download('stopwords')\n\ndef remove_stopwords(tokens):\n    stop_words = set(stopwords.words('english'))\n    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n    return filtered_tokens\n\n# Example\ntokens = ['this', 'is', 'a', 'sample', 'text', 'with', 'stop', 'words']\nfiltered = remove_stopwords(tokens)\nprint(filtered)  # ['sample', 'text', 'stop', 'words']\n"})}),"\n",(0,o.jsx)(n.h2,{id:"5-stemming-and-lemmatization",children:"5. Stemming and Lemmatization"}),"\n",(0,o.jsx)(n.h3,{id:"stemming",children:"Stemming"}),"\n",(0,o.jsx)(n.p,{children:"Stemming reduces words to their root form by removing suffixes:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"from nltk.stem import PorterStemmer\n\ndef stem_words(tokens):\n    stemmer = PorterStemmer()\n    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n    return stemmed_tokens\n\n# Example\ntokens = ['running', 'runs', 'ran', 'easily', 'fairly']\nstemmed = stem_words(tokens)\nprint(stemmed)  # ['run', 'run', 'ran', 'easili', 'fairli']\n"})}),"\n",(0,o.jsx)(n.h3,{id:"lemmatization",children:"Lemmatization"}),"\n",(0,o.jsx)(n.p,{children:"Lemmatization reduces words to their dictionary form (lemma):"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"from nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nimport nltk\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\ndef get_wordnet_pos(word):\n    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n    tag = nltk.pos_tag([word])[0][1][0].upper()\n    tag_dict = {\"J\": wordnet.ADJ,\n                \"N\": wordnet.NOUN,\n                \"V\": wordnet.VERB,\n                \"R\": wordnet.ADV}\n    \n    return tag_dict.get(tag, wordnet.NOUN)\n\ndef lemmatize_words(tokens):\n    lemmatizer = WordNetLemmatizer()\n    lemmatized_tokens = [\n        lemmatizer.lemmatize(token, get_wordnet_pos(token)) \n        for token in tokens\n    ]\n    return lemmatized_tokens\n\n# Example\ntokens = ['running', 'runs', 'ran', 'easily', 'fairly']\nlemmatized = lemmatize_words(tokens)\nprint(lemmatized)  # ['running', 'run', 'run', 'easily', 'fairly']\n"})}),"\n",(0,o.jsx)(n.h2,{id:"6-advanced-text-processing",children:"6. Advanced Text Processing"}),"\n",(0,o.jsx)(n.h3,{id:"n-gram-generation",children:"N-gram Generation"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"from nltk import ngrams\n\ndef generate_ngrams(tokens, n):\n    n_grams = list(ngrams(tokens, n))\n    return [' '.join(gram) for gram in n_grams]\n\n# Example\ntokens = ['natural', 'language', 'processing', 'is', 'important']\nbigrams = generate_ngrams(tokens, 2)\ntrigrams = generate_ngrams(tokens, 3)\n\nprint(bigrams)    # ['natural language', 'language processing', 'processing is', 'is important']\nprint(trigrams)   # ['natural language processing', 'language processing is', 'processing is important']\n"})}),"\n",(0,o.jsx)(n.h3,{id:"text-normalization-pipeline",children:"Text Normalization Pipeline"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'def text_normalization_pipeline(text):\n    # Clean text\n    cleaned_text = clean_text(text)\n    \n    # Normalize case\n    normalized_text = normalize_case(cleaned_text)\n    \n    # Tokenize\n    tokens = word_tokenize(normalized_text)\n    \n    # Remove stopwords\n    filtered_tokens = remove_stopwords(tokens)\n    \n    # Lemmatize\n    lemmatized_tokens = lemmatize_words(filtered_tokens)\n    \n    # Remove empty tokens and rejoin\n    final_tokens = [token for token in lemmatized_tokens if token.strip()]\n    \n    return \' \'.join(final_tokens)\n\n# Example\ntext = "This is a SAMPLE text with various elements! Don\'t miss it."\nprocessed = text_normalization_pipeline(text)\nprint(processed)  # "sample text various element do not miss"\n'})}),"\n",(0,o.jsx)(n.h2,{id:"7-handling-special-cases",children:"7. Handling Special Cases"}),"\n",(0,o.jsx)(n.h3,{id:"handling-negations",children:"Handling Negations"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"def handle_negation(tokens):\n    \"\"\"Preserve negation patterns\"\"\"\n    i = 0\n    processed_tokens = []\n    \n    while i < len(tokens):\n        token = tokens[i]\n        \n        if token == 'not' and i + 1 < len(tokens):\n            # Combine with next token\n            processed_tokens.append(f\"not_{tokens[i+1]}\")\n            i += 2  # Skip next token\n        else:\n            processed_tokens.append(token)\n            i += 1\n    \n    return processed_tokens\n\n# Example\ntokens = ['I', 'am', 'not', 'happy', 'about', 'this']\nnegation_handled = handle_negation(tokens)\nprint(negation_handled)  # ['I', 'am', 'not_happy', 'about', 'this']\n"})}),"\n",(0,o.jsx)(n.h3,{id:"spell-correction",children:"Spell Correction"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from textblob import TextBlob\n\ndef correct_spelling(text):\n    blob = TextBlob(text)\n    corrected = blob.correct()\n    return str(corrected)\n\n# Example\ntext = "I havv goood speling"\ncorrected = correct_spelling(text)\nprint(corrected)  # "I have good spelling"\n'})}),"\n",(0,o.jsx)(n.h2,{id:"best-practices-for-text-processing",children:"Best Practices for Text Processing"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Preserve Original Data"}),": Keep a copy of the original text when possible"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Consider Context"}),": Some processing steps might remove important context"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Domain-Specific Processing"}),": Adapt techniques to your specific domain"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Iterative Approach"}),": Start simple and add complexity as needed"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Validation"}),": Always validate the results of your preprocessing steps"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Performance"}),": Consider computational efficiency for large datasets"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"These text processing techniques form the backbone of NLP applications, preparing raw text for downstream tasks like classification, sentiment analysis, and information extraction."})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var i=t(6540);const o={},s=i.createContext(o);function r(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);
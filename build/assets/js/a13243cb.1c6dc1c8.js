"use strict";(globalThis.webpackChunkai_textbook=globalThis.webpackChunkai_textbook||[]).push([[9626],{6964:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>a,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"ml/supervised-learning","title":"Supervised Learning","description":"Supervised learning is a type of machine learning where the model is trained on a labeled dataset, meaning the input data is paired with the correct output. The goal is to learn a mapping from inputs to outputs that can be used to predict the output for new, unseen inputs.","source":"@site/docs/ml/supervised-learning.md","sourceDirName":"ml","slug":"/ml/supervised-learning","permalink":"/ai-textbook/docs/ml/supervised-learning","draft":false,"unlisted":false,"editUrl":"https://github.com/ai-textbook/ai-textbook/edit/main/docs/ml/supervised-learning.md","tags":[],"version":"current","frontMatter":{"sidebar_label":"Supervised Learning"},"sidebar":"tutorialSidebar","previous":{"title":"Introduction to ML","permalink":"/ai-textbook/docs/ml/introduction"},"next":{"title":"Unsupervised Learning","permalink":"/ai-textbook/docs/ml/unsupervised-learning"}}');var s=i(4848),t=i(8453);const a={sidebar_label:"Supervised Learning"},o="Supervised Learning",l={},d=[{value:"Overview of Supervised Learning",id:"overview-of-supervised-learning",level:2},{value:"Key Components:",id:"key-components",level:3},{value:"Types of Supervised Learning Problems",id:"types-of-supervised-learning-problems",level:2},{value:"1. Classification",id:"1-classification",level:3},{value:"Binary Classification",id:"binary-classification",level:4},{value:"Multiclass Classification",id:"multiclass-classification",level:4},{value:"2. Regression",id:"2-regression",level:3},{value:"Common Supervised Learning Algorithms",id:"common-supervised-learning-algorithms",level:2},{value:"1. Linear Regression",id:"1-linear-regression",level:3},{value:"2. Logistic Regression",id:"2-logistic-regression",level:3},{value:"3. Decision Trees",id:"3-decision-trees",level:3},{value:"4. Random Forest",id:"4-random-forest",level:3},{value:"5. Support Vector Machines (SVM)",id:"5-support-vector-machines-svm",level:3},{value:"Key Concepts in Supervised Learning",id:"key-concepts-in-supervised-learning",level:2},{value:"1. Training, Validation, and Test Sets",id:"1-training-validation-and-test-sets",level:3},{value:"2. Cross-Validation",id:"2-cross-validation",level:3},{value:"3. Hyperparameter Tuning",id:"3-hyperparameter-tuning",level:3},{value:"Model Evaluation Metrics",id:"model-evaluation-metrics",level:2},{value:"Classification Metrics",id:"classification-metrics",level:3},{value:"Regression Metrics",id:"regression-metrics",level:3},{value:"Common Challenges in Supervised Learning",id:"common-challenges-in-supervised-learning",level:2},{value:"1. Overfitting and Underfitting",id:"1-overfitting-and-underfitting",level:3},{value:"2. Data Preprocessing",id:"2-data-preprocessing",level:3}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"supervised-learning",children:"Supervised Learning"})}),"\n",(0,s.jsx)(n.p,{children:"Supervised learning is a type of machine learning where the model is trained on a labeled dataset, meaning the input data is paired with the correct output. The goal is to learn a mapping from inputs to outputs that can be used to predict the output for new, unseen inputs."}),"\n",(0,s.jsx)(n.h2,{id:"overview-of-supervised-learning",children:"Overview of Supervised Learning"}),"\n",(0,s.jsx)(n.p,{children:"In supervised learning, the algorithm learns from a training dataset that contains both input features and the corresponding target values. The model adjusts its parameters to minimize the difference between its predictions and the true values."}),"\n",(0,s.jsx)(n.h3,{id:"key-components",children:"Key Components:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Input variables (X)"}),": Features or independent variables"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Output variable (y)"}),": Target or dependent variable"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Training dataset"}),": Examples with inputs and correct outputs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model"}),": Function that maps inputs to outputs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Loss function"}),": Measures the difference between predictions and true values"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"types-of-supervised-learning-problems",children:"Types of Supervised Learning Problems"}),"\n",(0,s.jsx)(n.h3,{id:"1-classification",children:"1. Classification"}),"\n",(0,s.jsx)(n.p,{children:"Predicting discrete class labels."}),"\n",(0,s.jsx)(n.h4,{id:"binary-classification",children:"Binary Classification"}),"\n",(0,s.jsx)(n.p,{children:"Predicting one of two possible classes."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Generate a binary classification dataset\nX, y = make_classification(n_samples=1000, n_features=4, n_redundant=0, \n                           n_informative=4, n_clusters_per_class=1, random_state=42)\n\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create and train the model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_pred = model.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f"Accuracy: {accuracy:.3f}")\nprint(classification_report(y_test, y_pred))\n'})}),"\n",(0,s.jsx)(n.h4,{id:"multiclass-classification",children:"Multiclass Classification"}),"\n",(0,s.jsx)(n.p,{children:"Predicting one of multiple possible classes."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\n\n# Load the iris dataset\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create and train the model\nclf = DecisionTreeClassifier(random_state=42)\nclf.fit(X_train, y_train)\n\n# Make predictions\ny_pred = clf.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f"Accuracy: {accuracy:.3f}")\nprint("\\nConfusion Matrix:")\nprint(confusion_matrix(y_test, y_pred))\n'})}),"\n",(0,s.jsx)(n.h3,{id:"2-regression",children:"2. Regression"}),"\n",(0,s.jsx)(n.p,{children:"Predicting continuous numerical values."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\n\n# Generate a regression dataset\nX, y = make_regression(n_samples=1000, n_features=1, noise=10, random_state=42)\n\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create and train the model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"Mean Squared Error: {mse:.3f}\")\nprint(f\"R\xb2 Score: {r2:.3f}\")\n\n# Visualize the results\nplt.figure(figsize=(10, 6))\nplt.scatter(X_test, y_test, color='blue', label='Actual', alpha=0.6)\nplt.scatter(X_test, y_pred, color='red', label='Predicted', alpha=0.6)\nplt.xlabel('Feature')\nplt.ylabel('Target')\nplt.title('Actual vs Predicted Values')\nplt.legend()\nplt.show()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"common-supervised-learning-algorithms",children:"Common Supervised Learning Algorithms"}),"\n",(0,s.jsx)(n.h3,{id:"1-linear-regression",children:"1. Linear Regression"}),"\n",(0,s.jsx)(n.p,{children:"Used for regression tasks, models the relationship between variables using a straight line."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from sklearn.linear_model import LinearRegression\nimport numpy as np\n\ndef linear_regression_example():\n    # Simple linear regression: y = mx + b\n    np.random.seed(42)\n    X = np.random.randn(100, 1)\n    y = 2 * X.flatten() + 1 + np.random.randn(100) * 0.5  # y = 2x + 1 + noise\n    \n    model = LinearRegression()\n    model.fit(X, y)\n    \n    print(f"Slope: {model.coef_[0]:.3f}")\n    print(f"Intercept: {model.intercept_:.3f}")\n    print(f"Equation: y = {model.coef_[0]:.3f}x + {model.intercept_:.3f}")\n\nlinear_regression_example()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"2-logistic-regression",children:"2. Logistic Regression"}),"\n",(0,s.jsx)(n.p,{children:"Despite its name, used for binary classification problems."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import make_classification\n\ndef logistic_regression_example():\n    # Generate binary classification data\n    X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, \n                               n_informative=2, random_state=42, n_clusters_per_class=1)\n    \n    model = LogisticRegression()\n    model.fit(X, y)\n    \n    # Get class probabilities\n    probabilities = model.predict_proba(X[:5])\n    predictions = model.predict(X[:5])\n    \n    print("First 5 predictions:")\n    for i, (pred, prob) in enumerate(zip(predictions, probabilities)):\n        print(f"Sample {i+1}: Predicted={pred}, Probability=[{prob[0]:.3f}, {prob[1]:.3f}]")\n\nlogistic_regression_example()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"3-decision-trees",children:"3. Decision Trees"}),"\n",(0,s.jsx)(n.p,{children:"Tree-like model of decisions and their possible consequences."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.tree import export_text\n\ndef decision_tree_example():\n    # Generate classification data\n    X, y = make_classification(n_samples=100, n_features=4, n_classes=2, random_state=42)\n    \n    model = DecisionTreeClassifier(max_depth=3, random_state=42)\n    model.fit(X, y)\n    \n    # Print the decision tree\n    tree_rules = export_text(model, feature_names=[f\'feature_{i}\' for i in range(X.shape[1])])\n    print(tree_rules)\n    \n    # Feature importance\n    feature_importance = model.feature_importances_\n    print("\\nFeature Importance:")\n    for i, importance in enumerate(feature_importance):\n        print(f"Feature {i}: {importance:.3f}")\n\ndecision_tree_example()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"4-random-forest",children:"4. Random Forest"}),"\n",(0,s.jsx)(n.p,{children:"Ensemble method using multiple decision trees."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\n\ndef random_forest_example():\n    # Generate classification data\n    X, y = make_classification(n_samples=1000, n_features=4, n_classes=2, random_state=42)\n    \n    model = RandomForestClassifier(n_estimators=100, random_state=42)\n    model.fit(X, y)\n    \n    # Predictions\n    predictions = model.predict(X[:5])\n    probabilities = model.predict_proba(X[:5])\n    \n    print("Random Forest Predictions:")\n    for i, (pred, prob) in enumerate(zip(predictions, probabilities)):\n        print(f"Sample {i+1}: Predicted={pred}, Probability=[{prob[0]:.3f}, {prob[1]:.3f}]")\n    \n    # Feature importance\n    feature_importance = model.feature_importances_\n    print("\\nFeature Importance:")\n    for i, importance in enumerate(feature_importance):\n        print(f"Feature {i}: {importance:.3f}")\n\nrandom_forest_example()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"5-support-vector-machines-svm",children:"5. Support Vector Machines (SVM)"}),"\n",(0,s.jsx)(n.p,{children:"Effective for both classification and regression, but especially powerful for classification."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from sklearn.svm import SVC\nfrom sklearn.datasets import make_classification\n\ndef svm_example():\n    # Generate classification data\n    X, y = make_classification(n_samples=100, n_features=2, n_redundant=0, \n                               n_informative=2, random_state=42, n_clusters_per_class=1)\n    \n    model = SVC(kernel='rbf', random_state=42)\n    model.fit(X, y)\n    \n    # Predictions\n    predictions = model.predict(X[:5])\n    print(\"SVM Predictions on first 5 samples:\", predictions)\n\nsvm_example()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"key-concepts-in-supervised-learning",children:"Key Concepts in Supervised Learning"}),"\n",(0,s.jsx)(n.h3,{id:"1-training-validation-and-test-sets",children:"1. Training, Validation, and Test Sets"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from sklearn.model_selection import train_test_split\n\n# Split data into train, validation, and test sets\ndef split_data(X, y, test_size=0.2, val_size=0.2):\n    # First split: separate test set\n    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n    \n    # Second split: separate train and validation sets\n    train_size = 1 - val_size\n    X_train, X_val, y_train, y_val = train_test_split(\n        X_temp, y_temp, test_size=1-train_size, random_state=42\n    )\n    \n    return X_train, X_val, X_test, y_train, y_val, y_test\n\n# Example usage with iris dataset\niris = load_iris()\nX, y = iris.data, iris.target\nX_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y)\n\nprint(f"Training set size: {X_train.shape[0]}")\nprint(f"Validation set size: {X_val.shape[0]}")\nprint(f"Test set size: {X_test.shape[0]}")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"2-cross-validation",children:"2. Cross-Validation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef cross_validation_example():\n    # Load data\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    \n    # Create model\n    model = RandomForestClassifier(n_estimators=10, random_state=42)\n    \n    # Perform 5-fold cross-validation\n    cv_scores = cross_val_score(model, X, y, cv=5)\n    \n    print(f"Cross-validation scores: {cv_scores}")\n    print(f"Average CV score: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})")\n\ncross_validation_example()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"3-hyperparameter-tuning",children:"3. Hyperparameter Tuning"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef hyperparameter_tuning_example():\n    # Load data\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    \n    # Define the model\n    model = RandomForestClassifier(random_state=42)\n    \n    # Define the parameter grid\n    param_grid = {\n        'n_estimators': [50, 100, 200],\n        'max_depth': [3, 5, 7, None],\n        'min_samples_split': [2, 5, 10]\n    }\n    \n    # Perform grid search with cross-validation\n    grid_search = GridSearchCV(\n        estimator=model,\n        param_grid=param_grid,\n        cv=5,\n        scoring='accuracy',\n        n_jobs=-1\n    )\n    \n    grid_search.fit(X, y)\n    \n    print(f\"Best parameters: {grid_search.best_params_}\")\n    print(f\"Best cross-validation score: {grid_search.best_score_:.3f}\")\n    \n    # Use the best model\n    best_model = grid_search.best_estimator_\n    return best_model\n\nbest_model = hyperparameter_tuning_example()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"model-evaluation-metrics",children:"Model Evaluation Metrics"}),"\n",(0,s.jsx)(n.h3,{id:"classification-metrics",children:"Classification Metrics"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef classification_metrics_example():\n    # Generate binary classification data\n    X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # Train model\n    model = RandomForestClassifier(random_state=42)\n    model.fit(X_train, y_train)\n    \n    # Make predictions\n    y_pred = model.predict(X_test)\n    \n    # Calculate metrics\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    \n    print(f\"Accuracy: {accuracy:.3f}\")\n    print(f\"Precision: {precision:.3f}\")\n    print(f\"Recall: {recall:.3f}\")\n    print(f\"F1 Score: {f1:.3f}\")\n    \n    # Confusion Matrix\n    from sklearn.metrics import confusion_matrix\n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    \n    cm = confusion_matrix(y_test, y_pred)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n    plt.title('Confusion Matrix')\n    plt.ylabel('Actual')\n    plt.xlabel('Predicted')\n    plt.show()\n\nclassification_metrics_example()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"regression-metrics",children:"Regression Metrics"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\ndef regression_metrics_example():\n    # Generate regression data\n    X, y = make_regression(n_samples=1000, n_features=1, noise=10, random_state=42)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # Train model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    # Make predictions\n    y_pred = model.predict(X_test)\n    \n    # Calculate metrics\n    mse = mean_squared_error(y_test, y_pred)\n    rmse = mean_squared_error(y_test, y_pred, squared=False)  # Root MSE\n    mae = mean_absolute_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n    \n    print(f"Mean Squared Error: {mse:.3f}")\n    print(f"Root Mean Squared Error: {rmse:.3f}")\n    print(f"Mean Absolute Error: {mae:.3f}")\n    print(f"R\xb2 Score: {r2:.3f}")\n\nregression_metrics_example()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"common-challenges-in-supervised-learning",children:"Common Challenges in Supervised Learning"}),"\n",(0,s.jsx)(n.h3,{id:"1-overfitting-and-underfitting",children:"1. Overfitting and Underfitting"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Overfitting"}),": Model learns training data too well, including noise"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Underfitting"}),": Model fails to capture the underlying pattern"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nimport matplotlib.pyplot as plt\n\ndef overfitting_underfitting_example():\n    # Generate data\n    np.random.seed(42)\n    X = np.linspace(0, 1, 100).reshape(-1, 1)\n    y = 1.5 * X.ravel() + np.sin(1.5 * np.pi * X.ravel()) + np.random.normal(scale=0.1, size=X.shape[0])\n    \n    # Create polynomial models of different degrees\n    degrees = [1, 4, 15]\n    \n    plt.figure(figsize=(16, 5))\n    for i, degree in enumerate(degrees):\n        # Create pipeline with polynomial features and linear regression\n        poly_reg = Pipeline([\n            ('poly', PolynomialFeatures(degree=degree)),\n            ('linear', LinearRegression())\n        ])\n        \n        # Fit the model\n        poly_reg.fit(X, y)\n        \n        # Make predictions\n        X_plot = np.linspace(0, 1, 100).reshape(-1, 1)\n        y_plot = poly_reg.predict(X_plot)\n        \n        # Plot\n        plt.subplot(1, 3, i+1)\n        plt.scatter(X, y, alpha=0.5)\n        plt.plot(X_plot, y_plot, color='red')\n        plt.title(f'Polynomial Degree {degree}')\n        plt.xlabel('X')\n        plt.ylabel('y')\n    \n    plt.tight_layout()\n    plt.show()\n\noverfitting_underfitting_example()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-data-preprocessing",children:"2. Data Preprocessing"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.datasets import make_classification\n\ndef preprocessing_example():\n    # Generate data\n    X, y = make_classification(n_samples=1000, n_features=4, random_state=42)\n    \n    # StandardScaler: removes mean and scales to unit variance\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    print(f"Original data - Mean: {X.mean(axis=0)}, Std: {X.std(axis=0)}")\n    print(f"Scaled data - Mean: {X_scaled.mean(axis=0)}, Std: {X_scaled.std(axis=0)}")\n    \n    # MinMaxScaler: scales features to a range (typically 0-1)\n    minmax_scaler = MinMaxScaler()\n    X_minmax = minmax_scaler.fit_transform(X)\n    \n    print(f"MinMax scaled data - Min: {X_minmax.min(axis=0)}, Max: {X_minmax.max(axis=0)}")\n\npreprocessing_example()\n'})}),"\n",(0,s.jsx)(n.p,{children:"Supervised learning algorithms form the basis of many practical machine learning applications, from email spam detection to medical diagnosis. Choosing the right algorithm, properly preprocessing data, and evaluating model performance are crucial for success in supervised learning tasks."})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var r=i(6540);const s={},t=r.createContext(s);function a(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);
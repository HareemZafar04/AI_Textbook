"use strict";(globalThis.webpackChunkai_textbook=globalThis.webpackChunkai_textbook||[]).push([[2026],{2371:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>c,metadata:()=>t,toc:()=>a});const t=JSON.parse('{"id":"cv/object-detection","title":"Object Detection","description":"Object detection is a computer vision technique that combines image classification and localization to identify and locate objects within an image. Unlike image classification which only identifies what objects are present, object detection also determines where these objects are located in the image.","source":"@site/docs/cv/object-detection.md","sourceDirName":"cv","slug":"/cv/object-detection","permalink":"/ai-textbook/docs/cv/object-detection","draft":false,"unlisted":false,"editUrl":"https://github.com/ai-textbook/ai-textbook/edit/main/docs/cv/object-detection.md","tags":[],"version":"current","frontMatter":{"sidebar_label":"Object Detection"},"sidebar":"tutorialSidebar","previous":{"title":"Image Processing in Computer Vision","permalink":"/ai-textbook/docs/cv/image-processing"},"next":{"title":"Computer Vision Applications","permalink":"/ai-textbook/docs/cv/applications"}}');var s=i(4848),o=i(8453);const c={sidebar_label:"Object Detection"},r="Object Detection",l={},a=[{value:"What is Object Detection?",id:"what-is-object-detection",level:2},{value:"Object Detection vs. Image Classification vs. Image Segmentation",id:"object-detection-vs-image-classification-vs-image-segmentation",level:2},{value:"Key Concepts in Object Detection",id:"key-concepts-in-object-detection",level:2},{value:"1. Bounding Box",id:"1-bounding-box",level:3},{value:"2. Confidence Score",id:"2-confidence-score",level:3},{value:"3. Intersection over Union (IoU)",id:"3-intersection-over-union-iou",level:3},{value:"Traditional Object Detection Approaches",id:"traditional-object-detection-approaches",level:2},{value:"1. Sliding Window Approach",id:"1-sliding-window-approach",level:3},{value:"2. Selective Search",id:"2-selective-search",level:3},{value:"Modern Deep Learning Approaches",id:"modern-deep-learning-approaches",level:2},{value:"1. Two-Stage Detectors",id:"1-two-stage-detectors",level:3},{value:"R-CNN (Region-based CNN)",id:"r-cnn-region-based-cnn",level:4},{value:"Fast R-CNN",id:"fast-r-cnn",level:4},{value:"Faster R-CNN",id:"faster-r-cnn",level:4},{value:"2. Single-Stage Detectors",id:"2-single-stage-detectors",level:3},{value:"YOLO (You Only Look Once)",id:"yolo-you-only-look-once",level:4},{value:"SSD (Single Shot MultiBox Detector)",id:"ssd-single-shot-multibox-detector",level:4},{value:"RetinaNet",id:"retinanet",level:4},{value:"Evaluation Metrics",id:"evaluation-metrics",level:2},{value:"1. Mean Average Precision (mAP)",id:"1-mean-average-precision-map",level:3},{value:"2. Precision and Recall",id:"2-precision-and-recall",level:3},{value:"3. F1-Score",id:"3-f1-score",level:3},{value:"Popular Object Detection Architectures",id:"popular-object-detection-architectures",level:2},{value:"1. YOLO Series (v1 to v8)",id:"1-yolo-series-v1-to-v8",level:3},{value:"2. R-CNN Series",id:"2-r-cnn-series",level:3},{value:"3. EfficientDet",id:"3-efficientdet",level:3},{value:"Implementation Example with OpenCV and PyTorch",id:"implementation-example-with-opencv-and-pytorch",level:2},{value:"Applications of Object Detection",id:"applications-of-object-detection",level:2},{value:"Challenges in Object Detection",id:"challenges-in-object-detection",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"object-detection",children:"Object Detection"})}),"\n",(0,s.jsx)(n.p,{children:"Object detection is a computer vision technique that combines image classification and localization to identify and locate objects within an image. Unlike image classification which only identifies what objects are present, object detection also determines where these objects are located in the image."}),"\n",(0,s.jsx)(n.h2,{id:"what-is-object-detection",children:"What is Object Detection?"}),"\n",(0,s.jsx)(n.p,{children:"Object detection involves identifying specific objects in images and videos, and drawing bounding boxes around them. This technology is fundamental to many applications including autonomous vehicles, security systems, facial recognition, and robotics."}),"\n",(0,s.jsx)(n.h2,{id:"object-detection-vs-image-classification-vs-image-segmentation",children:"Object Detection vs. Image Classification vs. Image Segmentation"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Image Classification"}),": Identifies which class an entire image belongs to"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Object Detection"}),": Identifies objects in an image AND locates them with bounding boxes"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Image Segmentation"}),": Classifies each pixel in the image (more granular than object detection)"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"key-concepts-in-object-detection",children:"Key Concepts in Object Detection"}),"\n",(0,s.jsx)(n.h3,{id:"1-bounding-box",children:"1. Bounding Box"}),"\n",(0,s.jsx)(n.p,{children:"A rectangular box that defines the location of an object in an image, typically defined by coordinates (x, y) of the top-left corner and width (w) and height (h)."}),"\n",(0,s.jsx)(n.h3,{id:"2-confidence-score",children:"2. Confidence Score"}),"\n",(0,s.jsx)(n.p,{children:"A probability value between 0 and 1 indicating the model's confidence in its detection."}),"\n",(0,s.jsx)(n.h3,{id:"3-intersection-over-union-iou",children:"3. Intersection over Union (IoU)"}),"\n",(0,s.jsx)(n.p,{children:"A metric used to measure the overlap between predicted and ground truth bounding boxes:\nIoU = Area of Overlap / Area of Union"}),"\n",(0,s.jsx)(n.h2,{id:"traditional-object-detection-approaches",children:"Traditional Object Detection Approaches"}),"\n",(0,s.jsx)(n.h3,{id:"1-sliding-window-approach",children:"1. Sliding Window Approach"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Uses a fixed-size window that slides across the image"}),"\n",(0,s.jsx)(n.li,{children:"At each position, a classifier determines if the window contains an object"}),"\n",(0,s.jsx)(n.li,{children:"Computationally expensive due to multiple evaluations"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"2-selective-search",children:"2. Selective Search"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Generates region proposals based on image segmentation"}),"\n",(0,s.jsx)(n.li,{children:"More efficient than sliding window approach"}),"\n",(0,s.jsx)(n.li,{children:"Reduces the number of regions to evaluate"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"modern-deep-learning-approaches",children:"Modern Deep Learning Approaches"}),"\n",(0,s.jsx)(n.h3,{id:"1-two-stage-detectors",children:"1. Two-Stage Detectors"}),"\n",(0,s.jsx)(n.h4,{id:"r-cnn-region-based-cnn",children:"R-CNN (Region-based CNN)"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Selective search generates region proposals"}),"\n",(0,s.jsx)(n.li,{children:"Each region is warped to fixed size and processed by CNN"}),"\n",(0,s.jsx)(n.li,{children:"Time-consuming due to separate processing of each region"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Example of R-CNN approach concept\nimport cv2\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\n\nclass RCNN:\n    def __init__(self):\n        self.cnn_features = []\n        self.classifier = LogisticRegression()\n        \n    def selective_search_proposals(self, image):\n        # Use OpenCV's selective search implementation\n        ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n        ss.setBaseImage(image)\n        ss.switchToSelectiveSearchFast()\n        proposals = ss.process()\n        return proposals\n    \n    def extract_features(self, image, roi):\n        # Extract the region of interest\n        x, y, w, h = roi\n        roi_image = image[y:y+h, x:x+w]\n        \n        # Resize ROI to fixed size (for CNN)\n        resized_roi = cv2.resize(roi_image, (224, 224))\n        \n        # In a real implementation, you would pass through a CNN to extract features\n        # Here we'll simulate feature extraction\n        features = np.random.rand(1000)  # Placeholder for actual CNN features\n        \n        return features\n"})}),"\n",(0,s.jsx)(n.h4,{id:"fast-r-cnn",children:"Fast R-CNN"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Processes the entire image once through CNN"}),"\n",(0,s.jsx)(n.li,{children:"Uses region of interest (RoI) pooling to map region proposals to feature maps"}),"\n",(0,s.jsx)(n.li,{children:"Significantly faster than R-CNN"}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"faster-r-cnn",children:"Faster R-CNN"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Introduces Region Proposal Network (RPN)"}),"\n",(0,s.jsx)(n.li,{children:"Jointly trains RPN and detection network"}),"\n",(0,s.jsx)(n.li,{children:"Near real-time performance"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"2-single-stage-detectors",children:"2. Single-Stage Detectors"}),"\n",(0,s.jsx)(n.h4,{id:"yolo-you-only-look-once",children:"YOLO (You Only Look Once)"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Treats object detection as a regression problem"}),"\n",(0,s.jsx)(n.li,{children:"Divides image into grid and predicts bounding boxes and class probabilities directly"}),"\n",(0,s.jsx)(n.li,{children:"Very fast but sometimes less accurate for small objects"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Conceptual example of YOLO approach\nclass YOLO:\n    def __init__(self, grid_size=13, num_classes=80):\n        self.grid_size = grid_size\n        self.num_classes = num_classes\n        self.num_boxes = 2  # Number of bounding boxes per grid cell\n        \n    def predict(self, image):\n        # In a real implementation, this would be a deep neural network\n        # Here we'll simulate the output\n        # Output shape: [grid_size, grid_size, num_boxes*5 + num_classes]\n        output = np.random.rand(self.grid_size, self.grid_size, \n                                self.num_boxes*5 + self.num_classes)\n        return output\n"})}),"\n",(0,s.jsx)(n.h4,{id:"ssd-single-shot-multibox-detector",children:"SSD (Single Shot MultiBox Detector)"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Uses multiple feature maps at different scales"}),"\n",(0,s.jsx)(n.li,{children:"Better at detecting objects of various sizes"}),"\n",(0,s.jsx)(n.li,{children:"Good balance between speed and accuracy"}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"retinanet",children:"RetinaNet"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Introduces Focal Loss to handle class imbalance"}),"\n",(0,s.jsx)(n.li,{children:"State-of-the-art accuracy for many tasks"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"evaluation-metrics",children:"Evaluation Metrics"}),"\n",(0,s.jsx)(n.h3,{id:"1-mean-average-precision-map",children:"1. Mean Average Precision (mAP)"}),"\n",(0,s.jsx)(n.p,{children:"The primary metric for object detection evaluation:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Combines precision and recall across different IoU thresholds"}),"\n",(0,s.jsx)(n.li,{children:"Higher mAP indicates better performance"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"2-precision-and-recall",children:"2. Precision and Recall"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Precision = True Positives / (True Positives + False Positives)"}),"\n",(0,s.jsx)(n.li,{children:"Recall = True Positives / (True Positives + False Negatives)"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"3-f1-score",children:"3. F1-Score"}),"\n",(0,s.jsx)(n.p,{children:"Harmonic mean of precision and recall: F1 = 2 * (Precision * Recall) / (Precision + Recall)"}),"\n",(0,s.jsx)(n.h2,{id:"popular-object-detection-architectures",children:"Popular Object Detection Architectures"}),"\n",(0,s.jsx)(n.h3,{id:"1-yolo-series-v1-to-v8",children:"1. YOLO Series (v1 to v8)"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Real-time object detection"}),"\n",(0,s.jsx)(n.li,{children:"Different versions improve accuracy and speed"}),"\n",(0,s.jsx)(n.li,{children:"Easy to implement and deploy"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"2-r-cnn-series",children:"2. R-CNN Series"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"High accuracy, slower speed"}),"\n",(0,s.jsx)(n.li,{children:"Good for applications requiring high precision"}),"\n",(0,s.jsx)(n.li,{children:"Multiple variations with trade-offs"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"3-efficientdet",children:"3. EfficientDet"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Efficient architecture based on compound scaling"}),"\n",(0,s.jsx)(n.li,{children:"Good accuracy-to-efficiency ratio"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"implementation-example-with-opencv-and-pytorch",children:"Implementation Example with OpenCV and PyTorch"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import cv2\nimport torch\nimport torchvision\nfrom torchvision import transforms\nimport numpy as np\n\ndef detect_objects(image_path, confidence_threshold=0.5):\n    # Load pre-trained model (Faster R-CNN with ResNet backbone)\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n    model.eval()\n    \n    # Load and preprocess image\n    image = cv2.imread(image_path)\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n    transform = transforms.Compose([\n        transforms.ToTensor()\n    ])\n    \n    input_tensor = transform(image_rgb)\n    input_batch = input_tensor.unsqueeze(0)  # Add batch dimension\n    \n    # Perform inference\n    with torch.no_grad():\n        output = model(input_batch)\n    \n    # Extract results with sufficient confidence\n    boxes = output[0]['boxes']\n    labels = output[0]['labels']\n    scores = output[0]['scores']\n    \n    # Filter by confidence threshold\n    valid_detections = scores >= confidence_threshold\n    \n    result = {\n        'boxes': boxes[valid_detections].numpy(),\n        'labels': labels[valid_detections].numpy(),\n        'scores': scores[valid_detections].numpy()\n    }\n    \n    return result\n\n# Example usage\n# results = detect_objects('sample_image.jpg')\n# print(f\"Detected {len(results['boxes'])} objects\")\n"})}),"\n",(0,s.jsx)(n.h2,{id:"applications-of-object-detection",children:"Applications of Object Detection"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Autonomous Vehicles"}),": Detecting pedestrians, vehicles, traffic signs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Security Systems"}),": Intrusion detection, facial recognition"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Retail"}),": Customer behavior analysis, inventory management"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Healthcare"}),": Medical imaging, cell detection"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Agriculture"}),": Crop monitoring, pest detection"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Manufacturing"}),": Quality control, defect detection"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"challenges-in-object-detection",children:"Challenges in Object Detection"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Scale Variation"}),": Objects appear at different sizes"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Occlusion"}),": Objects partially hidden by other objects"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Illumination Changes"}),": Varying lighting conditions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Background Clutter"}),": Complex backgrounds making detection difficult"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Class Imbalance"}),": Unequal distribution of object classes"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-time Requirements"}),": Processing speed constraints"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Object detection continues to evolve with new architectures and techniques improving both accuracy and efficiency for diverse applications."})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>c,x:()=>r});var t=i(6540);const s={},o=t.createContext(s);function c(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:c(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);
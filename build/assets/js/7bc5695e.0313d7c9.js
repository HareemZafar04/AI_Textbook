"use strict";(globalThis.webpackChunkai_textbook=globalThis.webpackChunkai_textbook||[]).push([[4665],{3004:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>d,frontMatter:()=>r,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"ml/unsupervised-learning","title":"Unsupervised Learning","description":"Unsupervised learning is a type of machine learning where the model is trained on unlabeled data, meaning the input data does not have corresponding output labels. The goal is to discover hidden patterns, structures, or relationships in the data without explicit guidance.","source":"@site/docs/ml/unsupervised-learning.md","sourceDirName":"ml","slug":"/ml/unsupervised-learning","permalink":"/ai-textbook/docs/ml/unsupervised-learning","draft":false,"unlisted":false,"editUrl":"https://github.com/ai-textbook/ai-textbook/edit/main/docs/ml/unsupervised-learning.md","tags":[],"version":"current","frontMatter":{"sidebar_label":"Unsupervised Learning"},"sidebar":"tutorialSidebar","previous":{"title":"Supervised Learning","permalink":"/ai-textbook/docs/ml/supervised-learning"},"next":{"title":"Reinforcement Learning","permalink":"/ai-textbook/docs/ml/reinforcement-learning"}}');var s=t(4848),i=t(8453);const r={sidebar_label:"Unsupervised Learning"},l="Unsupervised Learning",o={},c=[{value:"Overview of Unsupervised Learning",id:"overview-of-unsupervised-learning",level:2},{value:"Key characteristics:",id:"key-characteristics",level:3},{value:"Types of Unsupervised Learning Problems",id:"types-of-unsupervised-learning-problems",level:2},{value:"1. Clustering",id:"1-clustering",level:3},{value:"K-Means Clustering",id:"k-means-clustering",level:4},{value:"Hierarchical Clustering",id:"hierarchical-clustering",level:4},{value:"DBSCAN (Density-Based Spatial Clustering)",id:"dbscan-density-based-spatial-clustering",level:4},{value:"2. Association Rule Learning",id:"2-association-rule-learning",level:3},{value:"Apriori Algorithm",id:"apriori-algorithm",level:4},{value:"3. Dimensionality Reduction",id:"3-dimensionality-reduction",level:3},{value:"Principal Component Analysis (PCA)",id:"principal-component-analysis-pca",level:4},{value:"t-SNE (t-distributed Stochastic Neighbor Embedding)",id:"t-sne-t-distributed-stochastic-neighbor-embedding",level:4},{value:"UMAP (Uniform Manifold Approximation and Projection)",id:"umap-uniform-manifold-approximation-and-projection",level:4},{value:"Feature Scaling and Preprocessing",id:"feature-scaling-and-preprocessing",level:2},{value:"Anomaly Detection",id:"anomaly-detection",level:2},{value:"Isolation Forest",id:"isolation-forest",level:3},{value:"Local Outlier Factor (LOF)",id:"local-outlier-factor-lof",level:3},{value:"Model Evaluation in Unsupervised Learning",id:"model-evaluation-in-unsupervised-learning",level:2},{value:"1. Silhouette Analysis",id:"1-silhouette-analysis",level:3},{value:"2. Elbow Method",id:"2-elbow-method",level:3},{value:"Unsupervised Learning Applications",id:"unsupervised-learning-applications",level:2},{value:"1. Customer Segmentation",id:"1-customer-segmentation",level:3},{value:"2. Document Clustering",id:"2-document-clustering",level:3},{value:"Challenges in Unsupervised Learning",id:"challenges-in-unsupervised-learning",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"unsupervised-learning",children:"Unsupervised Learning"})}),"\n",(0,s.jsx)(n.p,{children:"Unsupervised learning is a type of machine learning where the model is trained on unlabeled data, meaning the input data does not have corresponding output labels. The goal is to discover hidden patterns, structures, or relationships in the data without explicit guidance."}),"\n",(0,s.jsx)(n.h2,{id:"overview-of-unsupervised-learning",children:"Overview of Unsupervised Learning"}),"\n",(0,s.jsx)(n.p,{children:"Unlike supervised learning, unsupervised learning algorithms work with input data without target variables. The model must find patterns and structure in the data on its own. This makes unsupervised learning particularly useful for exploration and understanding complex datasets."}),"\n",(0,s.jsx)(n.h3,{id:"key-characteristics",children:"Key characteristics:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"No labeled output data"}),"\n",(0,s.jsx)(n.li,{children:"Discovery of hidden patterns"}),"\n",(0,s.jsx)(n.li,{children:"Often used for data preprocessing and exploration"}),"\n",(0,s.jsx)(n.li,{children:'Evaluation can be challenging as there are no "correct" answers'}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"types-of-unsupervised-learning-problems",children:"Types of Unsupervised Learning Problems"}),"\n",(0,s.jsx)(n.h3,{id:"1-clustering",children:"1. Clustering"}),"\n",(0,s.jsx)(n.p,{children:"Grouping similar instances together based on their features."}),"\n",(0,s.jsx)(n.h4,{id:"k-means-clustering",children:"K-Means Clustering"}),"\n",(0,s.jsx)(n.p,{children:"One of the most popular clustering algorithms that partitions data into K distinct clusters."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate sample data\nX, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=42)\n\n# Apply K-Means clustering\nkmeans = KMeans(n_clusters=4, random_state=42)\ny_pred = kmeans.fit_predict(X)\n\n# Plot the clustering results\nplt.figure(figsize=(10, 8))\nplt.scatter(X[:, 0], X[:, 1], c=y_pred, s=50, cmap='viridis')\ncenters = kmeans.cluster_centers_\nplt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75, marker='x')\nplt.title('K-Means Clustering')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.show()\n\nprint(f\"Cluster centers:\\n{centers}\")\n"})}),"\n",(0,s.jsx)(n.h4,{id:"hierarchical-clustering",children:"Hierarchical Clustering"}),"\n",(0,s.jsx)(n.p,{children:"Creates a tree of clusters by iteratively merging or splitting clusters."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from sklearn.cluster import AgglomerativeClustering\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom scipy.spatial.distance import pdist\nimport matplotlib.pyplot as plt\n\n# Generate sample data\nX, _ = make_blobs(n_samples=50, centers=4, cluster_std=1.0, random_state=42)\n\n# Perform hierarchical clustering\nlinkage_matrix = linkage(X, method='ward')\n\n# Create dendrogram\nplt.figure(figsize=(10, 6))\ndendrogram(linkage_matrix)\nplt.title('Hierarchical Clustering Dendrogram')\nplt.xlabel('Sample Index')\nplt.ylabel('Distance')\nplt.show()\n\n# Perform Agglomerative Clustering\nhierarchical = AgglomerativeClustering(n_clusters=4)\ny_pred = hierarchical.fit_predict(X)\n\nplt.figure(figsize=(10, 6))\nplt.scatter(X[:, 0], X[:, 1], c=y_pred, s=50, cmap='viridis')\nplt.title('Hierarchical Clustering Results')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.show()\n"})}),"\n",(0,s.jsx)(n.h4,{id:"dbscan-density-based-spatial-clustering",children:"DBSCAN (Density-Based Spatial Clustering)"}),"\n",(0,s.jsx)(n.p,{children:"Groups together points that are closely packed together."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_blobs\n\n# Create data with irregular clusters\nX, _ = make_blobs(n_samples=300, centers=4, n_features=2, random_state=42)\n# Add some noise\nrng = np.random.RandomState(42)\nX = np.vstack([X, rng.uniform(low=-6, high=6, size=(20, 2))])\n\n# Apply DBSCAN\ndbscan = DBSCAN(eps=0.5, min_samples=5)\ny_pred = dbscan.fit_predict(X)\n\nplt.figure(figsize=(10, 6))\nplt.scatter(X[:, 0], X[:, 1], c=y_pred, s=50, cmap='viridis')\nplt.title('DBSCAN Clustering')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.show()\n\n# Count points in each cluster and noise points\nunique_labels, counts = np.unique(y_pred, return_counts=True)\nprint(f\"Cluster labels: {unique_labels}\")\nprint(f\"Number of points per cluster: {dict(zip(unique_labels, counts))}\")\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-association-rule-learning",children:"2. Association Rule Learning"}),"\n",(0,s.jsx)(n.p,{children:"Discovering interesting relationships between variables in large databases."}),"\n",(0,s.jsx)(n.h4,{id:"apriori-algorithm",children:"Apriori Algorithm"}),"\n",(0,s.jsx)(n.p,{children:"Commonly used for market basket analysis."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Using mlxtend library for apriori algorithm\n# Note: This requires installing mlxtend: pip install mlxtend\ntry:\n    from mlxtend.frequent_patterns import apriori\n    from mlxtend.frequent_patterns import association_rules\n    import pandas as pd\n    \n    # Create sample transaction data\n    transactions = [\n        ['Milk', 'Eggs', 'Bread', 'Cheese'],\n        ['Eggs', 'Bread'],\n        ['Milk', 'Bread'],\n        ['Eggs', 'Bread', 'Butter'],\n        ['Milk', 'Eggs', 'Bread', 'Butter'],\n        ['Milk', 'Eggs', 'Butter'],\n        ['Bread', 'Butter'],\n        ['Milk', 'Eggs'],\n        ['Bread', 'Butter', 'Cheese'],\n        ['Milk', 'Bread', 'Cheese']\n    ]\n    \n    # Convert to transaction matrix\n    unique_items = set(item for transaction in transactions for item in transaction)\n    df = pd.DataFrame(\n        [[item in transaction for item in unique_items] for transaction in transactions],\n        columns=unique_items\n    )\n    \n    # Apply Apriori algorithm\n    frequent_itemsets = apriori(df, min_support=0.2, use_colnames=True)\n    \n    # Generate association rules\n    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.6)\n    \n    print(\"Frequent Itemsets:\")\n    print(frequent_itemsets)\n    print(\"\\nAssociation Rules:\")\n    print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n    \nexcept ImportError:\n    print(\"mlxtend library not available. Install with: pip install mlxtend\")\n"})}),"\n",(0,s.jsx)(n.h3,{id:"3-dimensionality-reduction",children:"3. Dimensionality Reduction"}),"\n",(0,s.jsx)(n.p,{children:"Reducing the number of features while preserving important information."}),"\n",(0,s.jsx)(n.h4,{id:"principal-component-analysis-pca",children:"Principal Component Analysis (PCA)"}),"\n",(0,s.jsx)(n.p,{children:"Linear dimensionality reduction technique."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from sklearn.decomposition import PCA\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\n\n# Load the iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Apply PCA to reduce to 2 dimensions\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\n# Plot the results\nplt.figure(figsize=(10, 6))\nscatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.7)\nplt.xlabel(f'First Principal Component ({pca.explained_variance_ratio_[0]:.2%} variance)')\nplt.ylabel(f'Second Principal Component ({pca.explained_variance_ratio_[1]:.2%} variance)')\nplt.title('PCA of Iris Dataset')\nplt.colorbar(scatter)\nplt.show()\n\nprint(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\nprint(f\"Total variance explained: {sum(pca.explained_variance_ratio_):.2%}\")\n"})}),"\n",(0,s.jsx)(n.h4,{id:"t-sne-t-distributed-stochastic-neighbor-embedding",children:"t-SNE (t-distributed Stochastic Neighbor Embedding)"}),"\n",(0,s.jsx)(n.p,{children:"Non-linear dimensionality reduction technique especially good for visualizing high-dimensional data."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from sklearn.manifold import TSNE\nfrom sklearn.datasets import load_digits\nimport matplotlib.pyplot as plt\n\n# Load the digits dataset\ndigits = load_digits()\nX = digits.data\ny = digits.target\n\n# Apply t-SNE to reduce to 2 dimensions\ntsne = TSNE(n_components=2, random_state=42, perplexity=30)\nX_tsne = tsne.fit_transform(X)\n\n# Plot the results\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10', alpha=0.7)\nplt.title('t-SNE Visualization of Digits Dataset')\nplt.colorbar(scatter)\nplt.xlabel('TSNE Component 1')\nplt.ylabel('TSNE Component 2')\nplt.show()\n"})}),"\n",(0,s.jsx)(n.h4,{id:"umap-uniform-manifold-approximation-and-projection",children:"UMAP (Uniform Manifold Approximation and Projection)"}),"\n",(0,s.jsx)(n.p,{children:"Modern technique for dimensionality reduction and visualization."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Note: This requires installing umap-learn: pip install umap-learn\ntry:\n    import umap\n    from sklearn.datasets import load_digits\n    import matplotlib.pyplot as plt\n\n    # Load the digits dataset\n    digits = load_digits()\n    X = digits.data\n    y = digits.target\n\n    # Apply UMAP\n    reducer = umap.UMAP(n_neighbors=5, min_dist=0.3, random_state=42)\n    X_umap = reducer.fit_transform(X)\n\n    # Plot the results\n    plt.figure(figsize=(10, 8))\n    scatter = plt.scatter(X_umap[:, 0], X_umap[:, 1], c=y, cmap='tab10', alpha=0.7)\n    plt.title('UMAP Visualization of Digits Dataset')\n    plt.colorbar(scatter)\n    plt.xlabel('UMAP Component 1')\n    plt.ylabel('UMAP Component 2')\n    plt.show()\nexcept ImportError:\n    print(\"umap-learn library not available. Install with: pip install umap-learn\")\n"})}),"\n",(0,s.jsx)(n.h2,{id:"feature-scaling-and-preprocessing",children:"Feature Scaling and Preprocessing"}),"\n",(0,s.jsx)(n.p,{children:"Many unsupervised algorithms are sensitive to the scale of the features."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\n# Generate sample data with different scales\nnp.random.seed(42)\nX = np.column_stack([\n    np.random.normal(0, 10, 100),  # Feature with large variance\n    np.random.normal(0, 1, 100)   # Feature with small variance\n])\n\nprint(f\"Before scaling - Feature 1: mean={X[:, 0].mean():.2f}, std={X[:, 0].std():.2f}\")\nprint(f\"Before scaling - Feature 2: mean={X[:, 1].mean():.2f}, std={X[:, 1].std():.2f}\")\n\n# Apply StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nprint(f\"\\nAfter scaling - Feature 1: mean={X_scaled[:, 0].mean():.2f}, std={X_scaled[:, 0].std():.2f}\")\nprint(f\"After scaling - Feature 2: mean={X_scaled[:, 1].mean():.2f}, std={X_scaled[:, 1].std():.2f}\")\n\n# Compare clustering results with and without scaling\nkmeans_before = KMeans(n_clusters=3, random_state=42)\nlabels_before = kmeans_before.fit_predict(X)\n\nkmeans_after = KMeans(n_clusters=3, random_state=42)\nlabels_after = kmeans_after.fit_predict(X_scaled)\n\n# Plot comparison\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\nax1.scatter(X[:, 0], X[:, 1], c=labels_before, cmap='viridis')\nax1.set_title('K-Means without Scaling')\nax1.set_xlabel('Feature 1')\nax1.set_ylabel('Feature 2')\n\nax2.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels_after, cmap='viridis')\nax2.set_title('K-Means with Scaling')\nax2.set_xlabel('Feature 1 (scaled)')\nax2.set_ylabel('Feature 2 (scaled)')\n\nplt.tight_layout()\nplt.show()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"anomaly-detection",children:"Anomaly Detection"}),"\n",(0,s.jsx)(n.p,{children:"Identifying unusual data points that do not conform to expected patterns."}),"\n",(0,s.jsx)(n.h3,{id:"isolation-forest",children:"Isolation Forest"}),"\n",(0,s.jsx)(n.p,{children:"Effective for anomaly detection in high-dimensional datasets."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from sklearn.ensemble import IsolationForest\nfrom sklearn.datasets import make_blobs\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create normal data\nX_normal, _ = make_blobs(n_samples=300, centers=1, n_features=2, \n                         random_state=42, cluster_std=1.0)\n\n# Add some anomalies\nX_anomaly = np.random.uniform(low=-6, high=6, size=(20, 2))\nX = np.vstack([X_normal, X_anomaly])\n\n# Apply Isolation Forest\niso_forest = IsolationForest(contamination=0.1, random_state=42)\nanomaly_labels = iso_forest.fit_predict(X)  # -1 for anomalies, 1 for normal\n\n# Plot results\nplt.figure(figsize=(10, 6))\nnormal_points = anomaly_labels == 1\nanomaly_points = anomaly_labels == -1\n\nplt.scatter(X[normal_points, 0], X[normal_points, 1], \n           c='blue', label='Normal', alpha=0.7)\nplt.scatter(X[anomaly_points, 0], X[anomaly_points, 1], \n           c='red', label='Anomaly', alpha=0.7)\nplt.title('Anomaly Detection with Isolation Forest')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.show()\n\nprint(f\"Number of anomalies detected: {sum(anomaly_points)}\")\nprint(f\"Expected anomalies: 20\")\n"})}),"\n",(0,s.jsx)(n.h3,{id:"local-outlier-factor-lof",children:"Local Outlier Factor (LOF)"}),"\n",(0,s.jsx)(n.p,{children:"Identifies anomalies based on local density."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from sklearn.neighbors import LocalOutlierFactor\n\n# Using the same data as above\nlof = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\nlof_labels = lof.fit_predict(X)\n\n# Plot results\nplt.figure(figsize=(10, 6))\nnormal_points = lof_labels == 1\nanomaly_points = lof_labels == -1\n\nplt.scatter(X[normal_points, 0], X[normal_points, 1], \n           c='blue', label='Normal', alpha=0.7)\nplt.scatter(X[anomaly_points, 0], X[anomaly_points, 1], \n           c='red', label='Anomaly', alpha=0.7)\nplt.title('Anomaly Detection with Local Outlier Factor')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.show()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"model-evaluation-in-unsupervised-learning",children:"Model Evaluation in Unsupervised Learning"}),"\n",(0,s.jsx)(n.p,{children:"Since there are no true labels, evaluation is more subjective:"}),"\n",(0,s.jsx)(n.h3,{id:"1-silhouette-analysis",children:"1. Silhouette Analysis"}),"\n",(0,s.jsx)(n.p,{children:"Measures how well-separated clusters are."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from sklearn.metrics import silhouette_score, silhouette_samples\nimport matplotlib.pyplot as plt\n\n# Generate sample data\nX, _ = make_blobs(n_samples=500, centers=4, cluster_std=1.0, random_state=42)\n\n# Try different numbers of clusters\nn_clusters_range = range(2, 8)\nsilhouette_scores = []\n\nfor n_clusters in n_clusters_range:\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    cluster_labels = kmeans.fit_predict(X)\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    silhouette_scores.append(silhouette_avg)\n    print(f\"For {n_clusters} clusters, silhouette score is: {silhouette_avg:.3f}\")\n\n# Plot silhouette scores\nplt.figure(figsize=(10, 6))\nplt.plot(n_clusters_range, silhouette_scores, marker='o')\nplt.title('Silhouette Score vs Number of Clusters')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Silhouette Score')\nplt.grid(True)\nplt.show()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-elbow-method",children:"2. Elbow Method"}),"\n",(0,s.jsx)(n.p,{children:"For determining the optimal number of clusters."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from sklearn.metrics import pairwise_distances\n\ndef calculate_wcss(X, max_k):\n    \"\"\"Calculate Within-Cluster Sum of Squares for different k values\"\"\"\n    wcss = []\n    for k in range(1, max_k + 1):\n        kmeans = KMeans(n_clusters=k, random_state=42)\n        kmeans.fit(X)\n        wcss.append(kmeans.inertia_)\n    return wcss\n\n# Calculate WCSS for different k values\nwcss = calculate_wcss(X, max_k=10)\n\n# Plot the elbow curve\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, len(wcss) + 1), wcss, marker='o')\nplt.title('Elbow Method for Optimal k')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Within-Cluster Sum of Squares (WCSS)')\nplt.grid(True)\nplt.show()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"unsupervised-learning-applications",children:"Unsupervised Learning Applications"}),"\n",(0,s.jsx)(n.h3,{id:"1-customer-segmentation",children:"1. Customer Segmentation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Create a sample customer dataset\nnp.random.seed(42)\nn_customers = 300\n\ndata = {\n    'Annual_Spending': np.random.normal(5000, 1500, n_customers),\n    'Frequency_of_Purchase': np.random.poisson(10, n_customers),\n    'Avg_Order_Value': np.random.normal(75, 25, n_customers)\n}\n\ncustomer_df = pd.DataFrame(data)\n\n# Preprocess the data\nscaler = StandardScaler()\ncustomer_scaled = scaler.fit_transform(customer_df)\n\n# Apply K-Means clustering\nkmeans = KMeans(n_clusters=4, random_state=42)\ncustomer_segments = kmeans.fit_predict(customer_scaled)\n\n# Add segments to the dataframe\ncustomer_df['Segment'] = customer_segments\n\n# Visualize customer segments\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.scatter(customer_df['Annual_Spending'], customer_df['Frequency_of_Purchase'], \n           c=customer_segments, cmap='viridis', alpha=0.7)\nplt.title('Customer Segments: Spending vs Frequency')\nplt.xlabel('Annual Spending')\nplt.ylabel('Frequency of Purchase')\n\nplt.subplot(1, 2, 2)\nplt.scatter(customer_df['Avg_Order_Value'], customer_df['Annual_Spending'], \n           c=customer_segments, cmap='viridis', alpha=0.7)\nplt.title('Customer Segments: Order Value vs Spending')\nplt.xlabel('Avg Order Value')\nplt.ylabel('Annual Spending')\n\nplt.tight_layout()\nplt.show()\n\n# Analyze each segment\nsegment_analysis = customer_df.groupby('Segment').agg({\n    'Annual_Spending': ['mean', 'std'],\n    'Frequency_of_Purchase': ['mean', 'std'],\n    'Avg_Order_Value': ['mean', 'std']\n}).round(2)\n\nprint(\"Customer Segment Analysis:\")\nprint(segment_analysis)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-document-clustering",children:"2. Document Clustering"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Sample documents\ndocuments = [\n    "The stock market showed significant growth today",\n    "New smartphone released with advanced camera features",\n    "Scientists discovered new species in the rainforest",\n    "Economic indicators suggest upcoming recession",\n    "Apple announces new iPhone model",\n    "Conservation efforts protect endangered animals",\n    "Stock prices fluctuate with market trends",\n    "Technology companies report quarterly earnings",\n    "Biologists study ecosystem diversity",\n    "Financial markets react to policy changes"\n]\n\n# Convert documents to TF-IDF vectors\nvectorizer = TfidfVectorizer(stop_words=\'english\', max_features=100)\nX = vectorizer.fit_transform(documents)\n\n# Apply K-Means clustering\nkmeans = KMeans(n_clusters=3, random_state=42)\ndocument_clusters = kmeans.fit_predict(X)\n\n# Reduce dimensions for visualization\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X.toarray())\n\n# Visualize document clusters\nplt.figure(figsize=(10, 6))\nscatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=document_clusters, cmap=\'viridis\', alpha=0.7)\n\n# Annotate points with document indices\nfor i, doc in enumerate(documents):\n    plt.annotate(str(i), (X_pca[i, 0], X_pca[i, 1]), fontsize=8, ha=\'center\')\n\nplt.title(\'Document Clustering with TF-IDF\')\nplt.xlabel(\'First Principal Component\')\nplt.ylabel(\'Second Principal Component\')\nplt.colorbar(scatter)\nplt.show()\n\n# Show which documents belong to each cluster\nfor cluster_id in range(3):\n    cluster_docs = [documents[i] for i, label in enumerate(document_clusters) if label == cluster_id]\n    print(f"\\nCluster {cluster_id}:")\n    for doc in cluster_docs:\n        print(f"  - {doc}")\n'})}),"\n",(0,s.jsx)(n.h2,{id:"challenges-in-unsupervised-learning",children:"Challenges in Unsupervised Learning"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"No Ground Truth"}),": Difficult to evaluate model performance objectively"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Parameter Selection"}),": Often requires domain knowledge to choose optimal parameters"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Interpretation"}),": Results may not always be intuitive or actionable"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Scalability"}),": Some unsupervised methods don't scale well with large datasets"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Unsupervised learning provides powerful tools for exploring and understanding data, identifying patterns, and preprocessing data for other machine learning tasks. The choice of algorithm depends on the specific problem and data characteristics."})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(p,{...e})}):p(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>l});var a=t(6540);const s={},i=a.createContext(s);function r(e){const n=a.useContext(i);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),a.createElement(i.Provider,{value:n},e.children)}}}]);
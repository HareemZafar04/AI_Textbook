"use strict";(globalThis.webpackChunkai_textbook=globalThis.webpackChunkai_textbook||[]).push([[6514],{5360:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>s,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"applications/robotics","title":"Robotics Applications","description":"Robotics is an interdisciplinary field that integrates mechanical engineering, electrical engineering, computer science, and artificial intelligence to design, construct, operate, and apply robots. This section explores the various applications of AI in robotics and their impact on different industries.","source":"@site/docs/applications/robotics.md","sourceDirName":"applications","slug":"/applications/robotics","permalink":"/ai-textbook/docs/applications/robotics","draft":false,"unlisted":false,"editUrl":"https://github.com/ai-textbook/ai-textbook/edit/main/docs/applications/robotics.md","tags":[],"version":"current","frontMatter":{"sidebar_label":"Robotics Applications"},"sidebar":"tutorialSidebar","previous":{"title":"autonomous-systems","permalink":"/ai-textbook/docs/applications/autonomous-systems"},"next":{"title":"AI Textbook Quiz","permalink":"/ai-textbook/docs/quiz"}}');var a=t(4848),i=t(8453);const s={sidebar_label:"Robotics Applications"},r="Robotics Applications",l={},c=[{value:"Overview of AI in Robotics",id:"overview-of-ai-in-robotics",level:2},{value:"Types of Robotic Systems",id:"types-of-robotic-systems",level:2},{value:"1. Industrial Robots",id:"1-industrial-robots",level:3},{value:"2. Service Robots",id:"2-service-robots",level:3},{value:"3. Mobile Robots",id:"3-mobile-robots",level:3},{value:"Perception and Computer Vision in Robots",id:"perception-and-computer-vision-in-robots",level:2},{value:"Robot Learning and Adaptation",id:"robot-learning-and-adaptation",level:2},{value:"Human-Robot Interaction",id:"human-robot-interaction",level:2},{value:"Multi-Robot Coordination",id:"multi-robot-coordination",level:2},{value:"Challenges in Robotic Applications",id:"challenges-in-robotic-applications",level:2},{value:"1. Environmental Uncertainty",id:"1-environmental-uncertainty",level:3},{value:"2. Safety and Reliability",id:"2-safety-and-reliability",level:3},{value:"3. Real-time Processing",id:"3-real-time-processing",level:3},{value:"4. Power Management",id:"4-power-management",level:3},{value:"5. Human-Robot Collaboration",id:"5-human-robot-collaboration",level:3},{value:"Future Directions",id:"future-directions",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"robotics-applications",children:"Robotics Applications"})}),"\n",(0,a.jsx)(n.p,{children:"Robotics is an interdisciplinary field that integrates mechanical engineering, electrical engineering, computer science, and artificial intelligence to design, construct, operate, and apply robots. This section explores the various applications of AI in robotics and their impact on different industries."}),"\n",(0,a.jsx)(n.h2,{id:"overview-of-ai-in-robotics",children:"Overview of AI in Robotics"}),"\n",(0,a.jsx)(n.p,{children:"AI enables robots to perceive their environment, make decisions, learn from experience, and interact with humans and other machines. The integration of AI with robotics has led to more autonomous, flexible, and capable robotic systems."}),"\n",(0,a.jsx)(n.h2,{id:"types-of-robotic-systems",children:"Types of Robotic Systems"}),"\n",(0,a.jsx)(n.h3,{id:"1-industrial-robots",children:"1. Industrial Robots"}),"\n",(0,a.jsx)(n.p,{children:"Automated machines designed to perform manufacturing tasks in industrial settings."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class IndustrialRobot:\n    def __init__(self, robot_id, robot_type):\n        self.robot_id = robot_id\n        self.robot_type = robot_type  # articulated, SCARA, delta, cartesian\n        self.position = [0, 0, 0]  # x, y, z coordinates\n        self.orientation = [0, 0, 0]  # roll, pitch, yaw\n        self.payload = 0  # maximum payload in kg\n        self.status = "idle"\n        self.task_queue = []\n        \n    def move_to_position(self, x, y, z, orientation=None):\n        """Move robot to specified position"""\n        self.position = [x, y, z]\n        if orientation:\n            self.orientation = orientation\n        self.status = "moving"\n        print(f"Robot {self.robot_id} moving to ({x}, {y}, {z})")\n        \n    def pick_object(self, weight):\n        """Simulate picking up an object"""\n        if weight <= self.payload:\n            self.status = "carrying"\n            print(f"Robot {self.robot_id} picked up object weighing {weight} kg")\n            return True\n        else:\n            print(f"Object too heavy for robot {self.robot_id}")\n            return False\n            \n    def place_object(self, x, y, z):\n        """Place object at specified location"""\n        self.status = "placing"\n        self.move_to_position(x, y, z)\n        self.status = "idle"\n        print(f"Robot {self.robot_id} placed object at ({x}, {y}, {z})")\n        \n    def add_task(self, task):\n        """Add task to robot\'s queue"""\n        self.task_queue.append(task)\n        \n    def execute_tasks(self):\n        """Execute queued tasks"""\n        for task in self.task_queue:\n            print(f"Executing task: {task}")\n            # Execute task logic here\n        self.task_queue = []\n\n# Example: Assembly line robot\nassembly_robot = IndustrialRobot("AR-001", "articulated")\nassembly_robot.payload = 5  # 5kg payload\nassembly_robot.add_task("Pick component A from bin 1")\nassembly_robot.add_task("Place component A on assembly station")\nassembly_robot.add_task("Tighten screws")\nassembly_robot.execute_tasks()\n'})}),"\n",(0,a.jsx)(n.h3,{id:"2-service-robots",children:"2. Service Robots"}),"\n",(0,a.jsx)(n.p,{children:"Designed to assist humans in various environments like homes, hospitals, and offices."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import datetime\n\nclass ServiceRobot:\n    def __init__(self, robot_name, environment):\n        self.name = robot_name\n        self.environment = environment  # "hospital", "home", "office"\n        self.battery_level = 100\n        self.location = "charging_station"\n        self.is_operational = True\n        self.task_log = []\n        \n    def navigate(self, destination):\n        """Navigate to destination using path planning"""\n        print(f"{self.name} navigating to {destination}")\n        # Simulate navigation\n        self.location = destination\n        self.battery_level -= 2  # Navigation consumes power\n        \n    def perform_service_task(self, task, location=None):\n        """Perform a service task like cleaning, delivery, etc."""\n        if not self.is_operational:\n            print(f"{self.name} is not operational")\n            return False\n            \n        if self.battery_level < 10:\n            print(f"{self.name} battery too low, returning to charging station")\n            self.return_to_charging_station()\n            return False\n            \n        if location:\n            self.navigate(location)\n            \n        print(f"{self.name} performing task: {task}")\n        \n        # Log the task\n        task_record = {\n            \'task\': task,\n            \'location\': self.location,\n            \'timestamp\': datetime.datetime.now(),\n            \'battery_after\': self.battery_level\n        }\n        self.task_log.append(task_record)\n        \n        # Performing task consumes battery\n        self.battery_level -= 3\n        \n        return True\n        \n    def return_to_charging_station(self):\n        """Return to charging station when battery is low"""\n        self.navigate("charging_station")\n        print(f"{self.name} returned to charging station")\n        \n    def charge_battery(self, charge_amount):\n        """Charge the robot\'s battery"""\n        self.battery_level = min(100, self.battery_level + charge_amount)\n        print(f"{self.name}\'s battery charged to {self.battery_level}%")\n\n# Example: Hospital service robot\nhospital_robot = ServiceRobot("MediBot-01", "hospital")\nhospital_robot.perform_service_task("Deliver medication", "Room 205")\nhospital_robot.perform_service_task("Transport medical supplies", "Lab A")\nhospital_robot.perform_service_task("Disinfect Room 205", "Room 205")\n\nprint(f"\\nTask log for {hospital_robot.name}:")\nfor i, task in enumerate(hospital_robot.task_log, 1):\n    print(f"{i}. {task[\'task\']} at {task[\'location\']} - "\n          f"{task[\'timestamp\'].strftime(\'%H:%M:%S\')}")\n'})}),"\n",(0,a.jsx)(n.h3,{id:"3-mobile-robots",children:"3. Mobile Robots"}),"\n",(0,a.jsx)(n.p,{children:"Robots capable of moving around in their environment."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport heapq\n\nclass MobileRobot:\n    def __init__(self, start_pos, goal_pos, environment_map):\n        self.position = np.array(start_pos, dtype=float)\n        self.goal = np.array(goal_pos, dtype=float)\n        self.environment = environment_map  # 2D grid: 0=free, 1=obstacle\n        self.path = []\n        self.battery = 100\n        self.max_speed = 1.0  # m/s\n        self.sensors = {\n            \'lidar\': True,\n            \'camera\': True,\n            \'imu\': True,\n            \'gps\': True\n        }\n        \n    def detect_obstacles(self):\n        """Detect obstacles using sensor data"""\n        # This would interface with real sensors in a real implementation\n        # For simulation, we\'ll use the environment map\n        grid_x, grid_y = int(self.position[0]), int(self.position[1])\n        \n        # Simple obstacle detection in adjacent cells\n        adjacent_positions = [\n            (grid_x-1, grid_y), (grid_x+1, grid_y),\n            (grid_x, grid_y-1), (grid_x, grid_y+1)\n        ]\n        \n        obstacles = []\n        for x, y in adjacent_positions:\n            if (0 <= x < self.environment.shape[0] and \n                0 <= y < self.environment.shape[1] and \n                self.environment[x, y] == 1):\n                obstacles.append((x, y))\n                \n        return obstacles\n        \n    def heuristic(self, a, b):\n        """Heuristic function for path planning"""\n        return np.sqrt((a[0] - b[0])**2 + (a[1] - b[1])**2)\n        \n    def plan_path(self):\n        """Plan a path to the goal using A* algorithm"""\n        start = tuple(self.position.astype(int))\n        goal = tuple(self.goal.astype(int))\n        \n        # A* implementation\n        frontier = [(0, start)]\n        came_from = {start: None}\n        cost_so_far = {start: 0}\n        \n        while frontier:\n            current_cost, current = heapq.heappop(frontier)\n            \n            if current == goal:\n                break\n                \n            for next_pos in self.get_neighbors(current):\n                if self.environment[next_pos] == 1:  # Obstacle\n                    continue\n                    \n                new_cost = cost_so_far[current] + 1  # Simple cost model\n                \n                if next_pos not in cost_so_far or new_cost < cost_so_far[next_pos]:\n                    cost_so_far[next_pos] = new_cost\n                    priority = new_cost + self.heuristic(goal, next_pos)\n                    heapq.heappush(frontier, (priority, next_pos))\n                    came_from[next_pos] = current\n        \n        # Reconstruct path\n        current = goal\n        path = [current]\n        while current != start:\n            current = came_from.get(current)\n            if current is None:\n                return []  # No path found\n            path.append(current)\n        path.reverse()\n        \n        self.path = path\n        return path\n        \n    def get_neighbors(self, pos):\n        """Get valid neighboring positions"""\n        x, y = pos\n        neighbors = [\n            (x-1, y), (x+1, y), (x, y-1), (x, y+1),  # 4-directional\n            (x-1, y-1), (x-1, y+1), (x+1, y-1), (x+1, y+1)  # Diagonal\n        ]\n        valid_neighbors = [\n            n for n in neighbors \n            if 0 <= n[0] < self.environment.shape[0] and \n               0 <= n[1] < self.environment.shape[1]\n        ]\n        return valid_neighbors\n        \n    def move_along_path(self):\n        """Move robot along the planned path"""\n        if not self.path:\n            print("No path to follow. Planning path...")\n            self.plan_path()\n            \n        if self.path:\n            next_pos = self.path.pop(0)\n            self.position = np.array(next_pos, dtype=float)\n            print(f"Moved to position: {self.position}")\n            self.battery -= 0.5  # Moving consumes battery\n            return True\n        else:\n            print("No path available to the goal")\n            return False\n\n# Example: Create a mobile robot in a simple environment\nenvironment = np.zeros((10, 10))  # 10x10 grid\n# Add some obstacles\nenvironment[3, 3:7] = 1  # Horizontal obstacle\nenvironment[5:8, 5] = 1  # Vertical obstacle\n\nmobile_robot = MobileRobot(start_pos=(1, 1), goal_pos=(8, 8), environment_map=environment)\n\nprint(f"Starting position: {mobile_robot.position}")\nprint(f"Goal position: {mobile_robot.goal}")\n\n# Plan path\npath = mobile_robot.plan_path()\nprint(f"Planned path: {path[:10]}...")  # Show first 10 steps\n\n# Move robot along path (first few steps)\nfor _ in range(5):\n    if mobile_robot.move_along_path():\n        print(f"Current position: {mobile_robot.position}")\n    else:\n        break\n'})}),"\n",(0,a.jsx)(n.h2,{id:"perception-and-computer-vision-in-robots",children:"Perception and Computer Vision in Robots"}),"\n",(0,a.jsx)(n.p,{children:"Robots need to perceive and understand their environment to operate effectively."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import cv2\nimport numpy as np\n\nclass RobotVisionSystem:\n    def __init__(self):\n        self.object_detection_model = None\n        self.calibrated = False\n        self.camera_matrix = None\n        self.dist_coeffs = None\n        \n    def calibrate_camera(self, calibration_images):\n        """Calibrate camera using chessboard images"""\n        # Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n        objp = np.zeros((6*9,3), np.float32)\n        objp[:,:2] = np.mgrid[0:9,0:6].T.reshape(-1,2)\n        \n        # Arrays to store object points and image points from all the images\n        objpoints = [] # 3d point in real world space\n        imgpoints = [] # 2d points in image plane\n        \n        for img in calibration_images:\n            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n            \n            # Find the chess board corners\n            ret, corners = cv2.findChessboardCorners(gray, (9,6), None)\n            \n            # If found, add object points, image points (after refining them)\n            if ret:\n                objpoints.append(objp)\n                imgpoints.append(corners)\n        \n        if len(objpoints) > 0:\n            ret, self.camera_matrix, self.dist_coeffs, _, _ = cv2.calibrateCamera(\n                objpoints, imgpoints, gray.shape[::-1], None, None)\n            self.calibrated = True\n            print("Camera calibrated successfully")\n        else:\n            print("Could not calibrate camera - no chessboards found in images")\n    \n    def detect_objects(self, image):\n        """Detect objects in an image"""\n        # Convert to grayscale\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        \n        # Simple color-based object detection\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n        \n        # Define range for red color (example)\n        lower_red = np.array([0, 50, 50])\n        upper_red = np.array([10, 255, 255])\n        lower_red2 = np.array([170, 50, 50])\n        upper_red2 = np.array([180, 255, 255])\n        \n        # Create masks\n        mask1 = cv2.inRange(hsv, lower_red, upper_red)\n        mask2 = cv2.inRange(hsv, lower_red2, upper_red2)\n        red_mask = cv2.bitwise_or(mask1, mask2)\n        \n        # Find contours\n        contours, _ = cv2.findContours(red_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        \n        objects = []\n        for contour in contours:\n            area = cv2.contourArea(contour)\n            if area > 1000:  # Filter small objects\n                # Get bounding box\n                x, y, w, h = cv2.boundingRect(contour)\n                \n                # Calculate center\n                center_x, center_y = x + w//2, y + h//2\n                \n                objects.append({\n                    \'type\': \'red_object\',\n                    \'bbox\': (x, y, w, h),\n                    \'center\': (center_x, center_y),\n                    \'area\': area\n                })\n        \n        return objects\n    \n    def estimate_depth(self, left_image, right_image):\n        """Estimate depth using stereo vision"""\n        # Create stereo matcher\n        stereo = cv2.StereoBM_create(numDisparities=16, blockSize=15)\n        \n        # Convert to grayscale\n        gray_left = cv2.cvtColor(left_image, cv2.COLOR_BGR2GRAY)\n        gray_right = cv2.cvtColor(right_image, cv2.COLOR_BGR2GRAY)\n        \n        # Compute disparity map\n        disparity = stereo.compute(gray_left, gray_right)\n        \n        # Convert to depth (simplified)\n        # In practice, you\'d use the camera parameters for accurate depth\n        depth_map = disparity.astype(np.float32) / 16.0\n        \n        return depth_map\n\n# Example: Robot vision system\nvision_system = RobotVisionSystem()\n\n# Create a sample image for demonstration\nsample_image = 255 * np.random.rand(480, 640, 3)\nsample_image = sample_image.astype(np.uint8)\n\n# Add a red rectangle (simulated object)\ncv2.rectangle(sample_image, (100, 100), (200, 200), (0, 0, 255), -1)\n\n# Detect objects\ndetected_objects = vision_system.detect_objects(sample_image)\nprint(f"Detected {len(detected_objects)} objects:")\nfor obj in detected_objects:\n    print(f"  {obj[\'type\']} at {obj[\'center\']} with area {obj[\'area\']:.0f}")\n'})}),"\n",(0,a.jsx)(n.h2,{id:"robot-learning-and-adaptation",children:"Robot Learning and Adaptation"}),"\n",(0,a.jsx)(n.p,{children:"Robots can learn and adapt to new situations using machine learning techniques."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nimport pickle\n\nclass RobotLearner:\n    def __init__(self):\n        self.model = RandomForestRegressor(n_estimators=100, random_state=42)\n        self.is_trained = False\n        self.training_data = []\n        self.performance_history = []\n        \n    def collect_training_data(self, state, action, reward, next_state):\n        """Collect experience for learning"""\n        experience = {\n            \'state\': state,\n            \'action\': action,\n            \'reward\': reward,\n            \'next_state\': next_state\n        }\n        self.training_data.append(experience)\n    \n    def train_model(self, target_variable=\'action_outcome\'):\n        """Train the robot\'s learning model"""\n        if len(self.training_data) < 10:\n            print("Not enough training data to train the model")\n            return False\n        \n        # Prepare features (state + action) and target (reward/outcome)\n        features = []\n        targets = []\n        \n        for exp in self.training_data:\n            # Combine state and action as features\n            state_action = exp[\'state\'] + [exp[\'action\']]\n            features.append(state_action)\n            \n            # Use reward as target variable\n            if target_variable == \'reward\':\n                targets.append(exp[\'reward\'])\n            else:  # Default to a combination of state transition quality\n                # Calculate how much closer to goal the action made the robot\n                current_distance = np.linalg.norm(np.array(exp[\'state\'][:2]))\n                next_distance = np.linalg.norm(np.array(exp[\'next_state\'][:2]))\n                distance_improvement = current_distance - next_distance\n                targets.append(distance_improvement)\n        \n        features = np.array(features)\n        targets = np.array(targets)\n        \n        # Split data\n        X_train, X_test, y_train, y_test = train_test_split(\n            features, targets, test_size=0.2, random_state=42\n        )\n        \n        # Train model\n        self.model.fit(X_train, y_train)\n        \n        # Evaluate model\n        train_score = self.model.score(X_train, y_train)\n        test_score = self.model.score(X_test, y_test)\n        \n        print(f"Model trained successfully!")\n        print(f"Training Score: {train_score:.4f}")\n        print(f"Testing Score: {test_score:.4f}")\n        \n        self.is_trained = True\n        return True\n    \n    def predict_outcome(self, state, action):\n        """Predict the outcome of taking an action in a given state"""\n        if not self.is_trained:\n            return 0  # Default prediction if not trained\n        \n        # Combine state and action\n        state_action = np.array(state + [action]).reshape(1, -1)\n        \n        # Predict outcome\n        prediction = self.model.predict(state_action)[0]\n        return prediction\n    \n    def learn_from_experience(self, state, action, reward, next_state):\n        """Complete learning cycle: collect data and potentially retrain"""\n        # Collect the new experience\n        self.collect_training_data(state, action, reward, next_state)\n        \n        # After collecting enough experiences, consider retraining\n        if len(self.training_data) % 20 == 0 and len(self.training_data) >= 20:\n            print("Retraining model with new experiences...")\n            self.train_model()\n            \n            # Store performance\n            recent_experiences = self.training_data[-10:]  # Last 10 experiences\n            avg_reward = np.mean([exp[\'reward\'] for exp in recent_experiences])\n            self.performance_history.append(avg_reward)\n    \n    def save_model(self, filepath):\n        """Save the trained model to file"""\n        if self.is_trained:\n            with open(filepath, \'wb\') as f:\n                pickle.dump({\n                    \'model\': self.model,\n                    \'training_data\': self.training_data,\n                    \'performance_history\': self.performance_history\n                }, f)\n            print(f"Model saved to {filepath}")\n        else:\n            print("No trained model to save")\n    \n    def load_model(self, filepath):\n        """Load a previously trained model from file"""\n        with open(filepath, \'rb\') as f:\n            saved_data = pickle.load(f)\n        \n        self.model = saved_data[\'model\']\n        self.training_data = saved_data[\'training_data\']\n        self.performance_history = saved_data[\'performance_history\']\n        self.is_trained = True\n        \n        print(f"Model loaded from {filepath}")\n\n# Example: Robot learning to navigate\nlearner = RobotLearner()\n\n# Simulate robot experiences\nfor episode in range(50):\n    # Simulate a simple navigation task\n    # State: [x_pos, y_pos, battery_level, obstacle_distance]\n    # Action: movement direction (0-3 for N, E, S, W)\n    # Reward: based on distance to goal and energy efficiency\n    \n    state = [np.random.uniform(-10, 10), np.random.uniform(-10, 10), \n             np.random.uniform(20, 100), np.random.uniform(0, 5)]\n    action = np.random.randint(0, 4)\n    reward = np.random.uniform(-1, 1)  # Can be positive or negative\n    next_state = [s + np.random.uniform(-1, 1) for s in state]  # Small change\n    \n    # Learn from this experience\n    learner.learn_from_experience(state, action, reward, next_state)\n\n# Make a prediction\nsample_state = [5, 5, 80, 2]  # At position (5,5), 80% battery, 2m from obstacle\nsample_action = 1  # Move East\npredicted_outcome = learner.predict_outcome(sample_state, sample_action)\nprint(f"Predicted outcome of moving East from {sample_state[:2]}: {predicted_outcome:.4f}")\n'})}),"\n",(0,a.jsx)(n.h2,{id:"human-robot-interaction",children:"Human-Robot Interaction"}),"\n",(0,a.jsx)(n.p,{children:"Robots need to effectively communicate and collaborate with humans."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import speech_recognition as sr\nimport pyttsx3\nimport datetime\nimport json\n\nclass HumanRobotInterface:\n    def __init__(self):\n        self.speech_recognizer = sr.Recognizer()\n        self.text_to_speech = pyttsx3.init()\n        \n        # Set TTS properties\n        self.text_to_speech.setProperty(\'rate\', 150)  # Speed of speech\n        self.text_to_speech.setProperty(\'volume\', 0.9)  # Volume level\n        \n        # Conversation history\n        self.conversation_history = []\n        self.robot_name = "Robo-Assist"\n        \n    def listen(self):\n        """Listen to user voice input"""\n        try:\n            with sr.Microphone() as source:\n                print("Listening...")\n                self.speech_recognizer.adjust_for_ambient_noise(source)\n                audio = self.speech_recognizer.listen(source, timeout=5)\n                \n            # Recognize speech using Google\'s speech recognition\n            text = self.speech_recognizer.recognize_google(audio)\n            print(f"You said: {text}")\n            return text\n            \n        except sr.WaitTimeoutError:\n            print("No speech detected")\n            return ""\n        except sr.UnknownValueError:\n            print("Could not understand audio")\n            return ""\n        except sr.RequestError as e:\n            print(f"Could not request results; {e}")\n            return ""\n    \n    def speak(self, text):\n        """Speak text to user"""\n        print(f"{self.robot_name}: {text}")\n        self.text_to_speech.say(text)\n        self.text_to_speech.runAndWait()\n        \n    def process_command(self, command):\n        """Process natural language commands"""\n        command = command.lower()\n        \n        # Time request\n        if "time" in command:\n            current_time = datetime.datetime.now().strftime("%I:%M %p")\n            response = f"The current time is {current_time}"\n            \n        # Date request\n        elif "date" in command:\n            current_date = datetime.datetime.now().strftime("%A, %B %d, %Y")\n            response = f"Today is {current_date}"\n            \n        # Greeting\n        elif any(greeting in command for greeting in ["hello", "hi", "hey"]):\n            response = f"Hello! I\'m {self.robot_name}, your robotic assistant. How can I help you today?"\n            \n        # Name inquiry\n        elif "your name" in command:\n            response = f"My name is {self.robot_name}. I\'m here to assist you."\n            \n        # Help request\n        elif "help" in command:\n            response = ("I can help with various tasks. You can ask me about the time, date, "\n                       "or just have a conversation. What would you like to know?")\n            \n        # Default response\n        else:\n            response = "I\'m sorry, I didn\'t understand that command. How else can I help you?"\n        \n        # Add to conversation history\n        self.conversation_history.append({\n            \'timestamp\': datetime.datetime.now().isoformat(),\n            \'user_input\': command,\n            \'robot_response\': response\n        })\n        \n        return response\n    \n    def initiate_conversation(self):\n        """Start a conversation with the user"""\n        self.speak(f"Hello! I\'m {self.robot_name}, your robotic assistant.")\n        \n        while True:\n            user_input = self.listen()\n            \n            if user_input:\n                if "stop" in user_input.lower() or "bye" in user_input.lower():\n                    self.speak("Goodbye! Have a great day!")\n                    break\n                else:\n                    response = self.process_command(user_input)\n                    self.speak(response)\n\n# Example: Voice interaction system\nhri_system = HumanRobotInterface()\n\n# Process a sample command (without actually listening to microphone)\nsample_command = "What time is it?"\nresponse = hri_system.process_command(sample_command)\nprint(f"Command: {sample_command}")\nprint(f"Response: {response}")\n\n# Another sample command\nsample_command = "Hello Robo-Assist"\nresponse = hri_system.process_command(sample_command)\nprint(f"Command: {sample_command}")\nprint(f"Response: {response}")\n'})}),"\n",(0,a.jsx)(n.h2,{id:"multi-robot-coordination",children:"Multi-Robot Coordination"}),"\n",(0,a.jsx)(n.p,{children:"Multiple robots can work together to accomplish complex tasks more efficiently."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import threading\nimport time\nimport random\n\nclass MultiRobotCoordinator:\n    def __init__(self, num_robots=3):\n        self.robots = []\n        self.tasks = []\n        self.task_assignments = {}\n        self.global_map = {}  # Shared map of environment\n        self.communication_channel = []  # Shared communication\n        self.lock = threading.Lock()  # For thread safety\n        \n        # Create robots\n        for i in range(num_robots):\n            robot = {\n                'id': f'Robot-{i}',\n                'position': [random.uniform(0, 10), random.uniform(0, 10)],\n                'status': 'idle',\n                'battery': 100,\n                'current_task': None\n            }\n            self.robots.append(robot)\n    \n    def add_task(self, task_id, location, priority='normal', task_type='delivery'):\n        \"\"\"Add a task to the system\"\"\"\n        task = {\n            'id': task_id,\n            'location': location,\n            'priority': priority,\n            'type': task_type,\n            'status': 'pending',\n            'assigned_robot': None\n        }\n        self.tasks.append(task)\n        self._broadcast_message(f\"New task {task_id} added at {location}\")\n        return task\n    \n    def assign_tasks(self):\n        \"\"\"Assign tasks to robots based on proximity and availability\"\"\"\n        # Filter unassigned tasks\n        unassigned_tasks = [task for task in self.tasks if task['status'] == 'pending']\n        \n        # Sort by priority then by task ID\n        priority_order = {'high': 3, 'normal': 2, 'low': 1}\n        unassigned_tasks.sort(key=lambda t: priority_order.get(t['priority'], 0), reverse=True)\n        \n        for task in unassigned_tasks:\n            # Find the closest available robot\n            available_robots = [r for r in self.robots if r['status'] == 'idle']\n            \n            if available_robots:\n                # Calculate distances and assign to closest robot\n                min_distance = float('inf')\n                closest_robot = None\n                \n                for robot in available_robots:\n                    distance = self._calculate_distance(robot['position'], task['location'])\n                    if distance < min_distance:\n                        min_distance = distance\n                        closest_robot = robot\n                \n                if closest_robot:\n                    # Assign task\n                    closest_robot['current_task'] = task['id']\n                    closest_robot['status'] = 'busy'\n                    task['assigned_robot'] = closest_robot['id']\n                    task['status'] = 'assigned'\n                    \n                    self.task_assignments[task['id']] = closest_robot['id']\n                    \n                    self._broadcast_message(\n                        f\"Task {task['id']} assigned to {closest_robot['id']}\"\n                    )\n    \n    def _calculate_distance(self, pos1, pos2):\n        \"\"\"Calculate Euclidean distance between two points\"\"\"\n        return ((pos1[0] - pos2[0])**2 + (pos1[1] - pos2[1])**2)**0.5\n    \n    def _broadcast_message(self, message):\n        \"\"\"Add message to shared communication channel\"\"\"\n        with self.lock:\n            self.communication_channel.append({\n                'timestamp': time.time(),\n                'message': message\n            })\n    \n    def execute_task(self, robot_id, task_id):\n        \"\"\"Simulate robot executing a task\"\"\"\n        robot = next(r for r in self.robots if r['id'] == robot_id)\n        task = next(t for t in self.tasks if t['id'] == task_id)\n        \n        robot['status'] = 'executing'\n        \n        # Simulate task execution time based on distance and task complexity\n        distance = self._calculate_distance(robot['position'], task['location'])\n        execution_time = distance * 0.5  # 0.5 seconds per unit distance\n        \n        # Simulate battery consumption\n        battery_consumption = execution_time * 2  # 2% per second\n        robot['battery'] = max(0, robot['battery'] - battery_consumption)\n        \n        # Update position to task location\n        robot['position'] = task['location']\n        \n        print(f\"{robot_id} executing task {task_id} at location {task['location']}\")\n        \n        # Simulate execution time\n        time.sleep(min(execution_time, 2))  # Cap execution time for demo\n        \n        # Mark task as completed\n        task['status'] = 'completed'\n        robot['status'] = 'returning'\n        \n        print(f\"{robot_id} completed task {task_id}\")\n        \n        # Return to base or charging station\n        robot['status'] = 'idle'\n        robot['current_task'] = None\n        \n        self._broadcast_message(f\"Task {task_id} completed by {robot_id}\")\n    \n    def run_coordinated_operation(self):\n        \"\"\"Run a coordinated multi-robot operation\"\"\"\n        # Add tasks\n        self.add_task(\"T001\", [5, 5], \"high\", \"delivery\")\n        self.add_task(\"T002\", [8, 2], \"normal\", \"inspection\")\n        self.add_task(\"T003\", [1, 9], \"low\", \"maintenance\")\n        \n        print(f\"Initial robot positions: {[(r['id'], r['position']) for r in self.robots]}\")\n        \n        # Assign tasks\n        self.assign_tasks()\n        \n        # Execute tasks in a coordinated manner\n        for task in self.tasks:\n            if task['status'] == 'assigned':\n                robot_id = task['assigned_robot']\n                # In a real system, this would be a separate thread for each robot\n                self.execute_task(robot_id, task['id'])\n        \n        # Print final status\n        print(\"\\nFinal robot status:\")\n        for robot in self.robots:\n            print(f\"  {robot['id']}: {robot['status']}, Battery: {robot['battery']:.1f}%, \"\n                  f\"Position: {robot['position']}\")\n        \n        # Print task status\n        print(f\"\\nTask Status:\")\n        for task in self.tasks:\n            print(f\"  {task['id']}: {task['status']}\")\n\n# Example: Coordinated multi-robot operation\ncoordinator = MultiRobotCoordinator(num_robots=3)\ncoordinator.run_coordinated_operation()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"challenges-in-robotic-applications",children:"Challenges in Robotic Applications"}),"\n",(0,a.jsx)(n.h3,{id:"1-environmental-uncertainty",children:"1. Environmental Uncertainty"}),"\n",(0,a.jsx)(n.p,{children:"Robots must operate in unpredictable environments with changing conditions."}),"\n",(0,a.jsx)(n.h3,{id:"2-safety-and-reliability",children:"2. Safety and Reliability"}),"\n",(0,a.jsx)(n.p,{children:"Ensuring robots operate safely around humans and other systems."}),"\n",(0,a.jsx)(n.h3,{id:"3-real-time-processing",children:"3. Real-time Processing"}),"\n",(0,a.jsx)(n.p,{children:"Making decisions and executing actions within strict time constraints."}),"\n",(0,a.jsx)(n.h3,{id:"4-power-management",children:"4. Power Management"}),"\n",(0,a.jsx)(n.p,{children:"Optimizing battery life and energy efficiency for autonomous operation."}),"\n",(0,a.jsx)(n.h3,{id:"5-human-robot-collaboration",children:"5. Human-Robot Collaboration"}),"\n",(0,a.jsx)(n.p,{children:"Designing robots that can effectively work alongside humans."}),"\n",(0,a.jsx)(n.h2,{id:"future-directions",children:"Future Directions"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Soft Robotics"}),": Creating robots with flexible, adaptable components"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Swarm Robotics"}),": Large groups of simple robots working together"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Bio-inspired Robotics"}),": Learning from biological systems"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Cloud Robotics"}),": Robots leveraging cloud-based AI and computation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Humanoid Robots"}),": More human-like robots for complex interactions"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Robotics continues to evolve with advances in AI, materials science, and mechanical engineering, enabling increasingly sophisticated applications across manufacturing, healthcare, agriculture, and service industries."})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>r});var o=t(6540);const a={},i=o.createContext(a);function s(e){const n=o.useContext(i);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);